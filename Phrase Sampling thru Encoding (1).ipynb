{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING AND LOADING A CORPUS\n",
    "\n",
    "https://nlpforhackers.io/corpora/\n",
    "http://lucumr.pocoo.org/2015/11/18/pythons-hidden-re-gems/\n",
    "\n",
    "The goal of this step is to develop an initial list of each character and their spoken lines, or a cleanish list of the lines within the text. (Dictionaries are Hash value arbitrary, so may not be ordered the same. Lists are used instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and Read a file into \"f\" using a list and stripping out all Project Gutenberg headers and footers\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "data_folder = Path(\"data/murray_kempton/\")\n",
    "file_to_open = data_folder / \"On_Frank_Sinatra.txt\"\n",
    "f = open(file_to_open, 'r')\n",
    "first_document = list(f)\n",
    "#print(first_document)\n",
    "\n",
    "# Determine whether a Project Gutenberg Text\n",
    "first_header_index = 0\n",
    "second_header_index = 0\n",
    "footer_index = 0\n",
    "if any(\"GUTENBERG\" in s for s in first_document):\n",
    "    for first_header_index in range( len(first_document) ):\n",
    "        if ( ( first_document[first_header_index].find('*END*THE SMALL PRINT!') ) != -1 ) :\n",
    "            break\n",
    "        else:\n",
    "            for first_header_index in range( len(first_document) ):\n",
    "                if ( ( first_document[first_header_index].find('START OF THIS PROJECT GUTENBERG') ) != -1 ) :\n",
    "                    break        \n",
    "        \n",
    "    second_document = list(first_document[first_header_index + 1 :])\n",
    "\n",
    "    for second_header_index in range( len(second_document) ):\n",
    "        if ( ( second_document[second_header_index].find('www.gutenberg.org') ) != -1 ) :\n",
    "            break            \n",
    "    for footer_index in range( len(first_document) ):\n",
    "        if ( ( first_document[footer_index].find('End of Project') ) != -1 ) :\n",
    "            break\n",
    "        else:\n",
    "            for footer_index in range( len(first_document) ):\n",
    "                if ( ( first_document[footer_index].find('End of the Project') ) != -1 ) :\n",
    "                    break    \n",
    "        \n",
    "    print(first_header_index)            \n",
    "    print(second_header_index)\n",
    "    print(footer_index)     \n",
    "    \n",
    "    script = list()\n",
    "    if (second_header_index < (first_header_index + 100)):\n",
    "        script = list(first_document[first_header_index +1 + second_header_index +1 : footer_index-1])\n",
    "    else:\n",
    "        script = list(first_document[first_header_index +1 : footer_index-1])\n",
    "else:\n",
    "    script = first_document\n",
    "\n",
    "#print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile a list of speakers\n",
    "r = re.compile(\"[A-Z0-9][A-Z0-9]+\")\n",
    "speakers = []\n",
    "for line in script:\n",
    "    mtch = r.match(line)\n",
    "    if mtch:\n",
    "        speakers.append(mtch.group())\n",
    "#print(speakers)\n",
    "\n",
    "#Omit speakers from the list of text\n",
    "s = re.compile(r\"\\b[A-Z{3}\\.]+\\b\")\n",
    "spoken = list(filter(lambda i: not r.search(i), script))\n",
    "\n",
    "#print(spoken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RE-CREATING SENTENCES AND PHRASES\n",
    "\n",
    "During this step, we concatenate lines in batches to allow the identification of sentences with regular expressions. Then we identify phrases with stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate lines into list entries for future sentence splitting\n",
    "\n",
    "newLines = []\n",
    "singleLine = ''\n",
    "singleLines = []\n",
    "\n",
    "#Remove all line returns\n",
    "for j in range(0, len(spoken)):\n",
    "    spoken[j] = spoken[j].replace('\\n', '')\n",
    "    \n",
    "#Split 5 lines at a time into new list\n",
    "for k in range( 0, len(spoken), 3):\n",
    "    newLines = []\n",
    "    for line in range( 0, 3 ):\n",
    "        try:\n",
    "            newLines.append(' '+spoken[line+k])\n",
    "        except:\n",
    "            #print(\"Index Error at\", k, line)\n",
    "            break\n",
    "    #Join 5-line groups into one line and append to a list\n",
    "    singleLine = ''.join(newLines)\n",
    "    singleLines.append(singleLine)\n",
    "\n",
    "#print(singleLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create list of sentences\n",
    "sentences = []\n",
    "for m in range(0, len(singleLines)):\n",
    "    mtch = re.findall(\"[A-Z][^\\.!?]*[\\.!?]\", singleLines[m], re.M|re.I)\n",
    "    if mtch:\n",
    "        sentences.append(mtch)\n",
    "\n",
    "#print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Clean the stopword list\n",
    "stoplist = []\n",
    "clean_line = []\n",
    "data_folder = Path(\"data/\")\n",
    "file_to_open = data_folder / \"snowball_stop.txt\"\n",
    "f = open(file_to_open, 'r')\n",
    "full_stop = list(f)\n",
    "\n",
    "for n in range( 0, len(full_stop), 1 ):\n",
    "    clean_line = full_stop[n].split('|')\n",
    "    stoplist.append(clean_line[0])\n",
    "\n",
    "for p in range(len(stoplist)):\n",
    "    stoplist[p] = stoplist[p].replace('\\n', '')\n",
    "    \n",
    "#print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank_Sinatra_ever\n",
      "fullest_duty\n",
      "leaving\n",
      "duty\n",
      "dealing\n",
      "subject\n",
      "Sinatra_came\n",
      "intruder-mistrusting_voice_suddenly_softened\n",
      "Just\n",
      "little_telling\n",
      "That’s\n",
      "have\n",
      "thing\n",
      "trash\n",
      "also_knew\n",
      "everything\n",
      "stolen\n",
      "have\n",
      "taste\n",
      "confine_your_larcenies\n",
      "appreciate\n",
      "work\n",
      "overlook_elements\n",
      "porkpie\n",
      "walk\n",
      "shadows\n",
      "loneliness\n",
      "light\n",
      "back\n",
      "taken_intact\n",
      "“One\n",
      "Baby”\n",
      "Fred_Astaire_consummated\n",
      "unjustly_forgotten_movie_Sky’s\n",
      "remember\n",
      "Sinatra_trying\n",
      "together\n",
      "sextet\n",
      "Kennedy_Inaugural\n",
      "finally\n",
      "you’ll_ever_learn\n",
      "sing\n",
      "listen\n",
      "Billie_Holiday\n",
      "find\n",
      "play\n",
      "Lord_given_voice\n",
      "mother\n",
      "magpie’s_cunning_account\n",
      "enduring_distinction\n"
     ]
    }
   ],
   "source": [
    "# Create list of phrases using stopwords\n",
    "phrases = []\n",
    "candidate_phrases = []\n",
    "\n",
    "for q in range(len(sentences)):\n",
    "    for r in sentences[q]:\n",
    "        words = re.split(\"\\\\s+\", r)\n",
    "        previous_stop = False\n",
    " \n",
    "        # Examine each word to determine if it is a phrase boundary marker or part of a phrase or alone\n",
    "        for w in words:\n",
    " \n",
    "            if w in stoplist and not previous_stop:\n",
    "                # phrase boundary encountered, so put a hard indicator\n",
    "                candidate_phrases.append(\";\")\n",
    "                previous_stop = True\n",
    "            elif w not in stoplist and len(w) > 3:\n",
    "                # keep adding words to list until a phrase boundary is detected\n",
    "                candidate_phrases.append(w.strip())\n",
    "                previous_stop = False\n",
    " \n",
    "    # Create a list of candidate phrases without boundary demarcation\n",
    "    phrases = re.split(\";+\", ' '.join(candidate_phrases))\n",
    "\n",
    "# Clean up phrases    \n",
    "re2 = re.compile('[^\\.!?,\"(){}\\*:]*[\\.!?,\"(){}\\*:]')\n",
    "for s in range(len(phrases)):\n",
    "    phrases[s] = re.sub(re2, '', phrases[s])\n",
    "    phrases[s] = phrases[s].strip(' ')\n",
    "    phrases[s] = phrases[s].replace(' ', '_')\n",
    "    phrases[s] = phrases[s].replace('__', '_')\n",
    "    phrases[s] = phrases[s].strip('_')\n",
    "\n",
    "for s in range(len(phrases)):\n",
    "    try:\n",
    "        phrases.remove('')\n",
    "        phrases.remove(' ')\n",
    "        phrases.remove('/n')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for t in range(50):\n",
    "    print(phrases[t])\n",
    "\n",
    "#print(phrases)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERM FREQUENCY–INVERSE DOCUMENT FREQUENCY (TF-IDF)\n",
    "![image.png](attachment:image.png)\n",
    "The quintessential early Natural Language Processing tool, the TF-IDF analysis for context and sentiment evaluation is useful only over a large corpus. It must be understood that the corpus is not just a sample to be evaluated, but instead is the entire population that sets a 'benchmark' for evaluation, if you will. \n",
    "\n",
    "Here we establish a Term Frequency (TF) count of word frequencies, just as we showed a phrase frequency count in the last step in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank\n",
      "Sinatra\n",
      "ever\n",
      "did\n",
      "the\n",
      "fullest\n",
      "duty\n",
      "to\n",
      "his\n",
      "art,\n",
      "and\n",
      "now\n",
      "he\n",
      "is\n",
      "leaving\n",
      "us\n",
      "with\n",
      "the\n",
      "duty\n",
      "to\n",
      "sum\n",
      "him\n",
      "up.\n",
      "My\n",
      "betters\n",
      "have\n",
      "already\n",
      "done\n",
      "that.\n",
      "One\n",
      "day\n",
      "I\n",
      "was\n",
      "dealing\n",
      "with\n",
      "Ella\n",
      "Fitzgerald,\n",
      "and\n",
      "the\n",
      "subject\n",
      "of\n",
      "Sinatra\n",
      "came\n",
      "up\n",
      "and\n",
      "her\n",
      "intruder-mistrusting\n",
      "voice\n",
      "suddenly\n",
      "softened\n"
     ]
    }
   ],
   "source": [
    "#Establish wordList\n",
    "wordList = []\n",
    "for u in range(len(sentences)):\n",
    "    for v in sentences[u]:\n",
    "        words = re.split(\"\\\\s+\", v)\n",
    "        wordList.extend(words)\n",
    "        \n",
    "for w in range(50):\n",
    "    print(wordList[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frank 2\n",
      "sinatra 1\n",
      "ever 3\n",
      "did 1\n",
      "the 7\n",
      "fullest 1\n",
      "duty 1\n",
      "to 16\n",
      "his 4\n",
      "art, 1\n",
      "and 4\n",
      "now 2\n",
      "he 65\n",
      "is 29\n",
      "leaving 1\n",
      "us 9\n",
      "with 1\n",
      "sum 2\n",
      "him 1\n",
      "up 2\n",
      "my 3\n",
      "betters 1\n",
      "have 1\n",
      "already 1\n",
      "done 1\n",
      "that 4\n",
      "one 6\n",
      "day 3\n",
      "i 181\n",
      "was 1\n",
      "dealing 1\n",
      "ella 1\n",
      "fitzgerald, 1\n",
      "subject 1\n",
      "of 2\n",
      "came 2\n",
      "her 11\n",
      "intruder-mistrusting 1\n",
      "voice 1\n",
      "suddenly 1\n",
      "softened 1\n",
      "she 1\n",
      "said, 1\n",
      "“frank 1\n",
      "just 2\n",
      "this 1\n",
      "little 1\n",
      "guy 1\n",
      "telling 1\n",
      "story 1\n",
      "that’s 1\n",
      "all 7\n",
      "you 1\n",
      "be 4\n",
      "for 3\n",
      "knew, 1\n",
      "as 21\n",
      "every 1\n",
      "artist 1\n",
      "must, 1\n",
      "there 1\n",
      "no 4\n",
      "such 1\n",
      "thing 2\n",
      "trash 1\n",
      "cannot 1\n",
      "transcended 1\n",
      "also 1\n",
      "knew 1\n",
      "second 1\n",
      "great 1\n",
      "lesson, 1\n",
      "which 1\n",
      "everything 1\n",
      "stolen 1\n",
      "if 4\n",
      "taste 1\n",
      "confine 1\n",
      "your 1\n",
      "larcenies 1\n",
      "worth-taking 1\n",
      "we 3\n",
      "appreciate 1\n",
      "work 1\n",
      "overlook 1\n",
      "its 1\n",
      "elements 1\n",
      "creative 1\n",
      "plagiarism 1\n",
      "porkpie 1\n",
      "hat 19\n",
      "walk 1\n",
      "into 1\n",
      "shadows 1\n",
      "loneliness 1\n",
      "light 2\n",
      "at 51\n",
      "back 1\n",
      "are 5\n",
      "taken 1\n",
      "intact 1\n",
      "from 1\n",
      "“one 1\n",
      "baby” 1\n",
      "fred 1\n",
      "astaire 1\n",
      "consummated 1\n",
      "in 55\n",
      "unjustly 1\n",
      "forgotten 1\n",
      "movie 1\n",
      "sky’s 1\n",
      "limit 1\n",
      "remember 1\n",
      "when 1\n",
      "trying 1\n",
      "put 1\n",
      "together 1\n",
      "a 186\n",
      "sextet 1\n",
      "kennedy 1\n",
      "inaugural 1\n",
      "finally 1\n",
      "had 2\n",
      "tell 1\n",
      "milton 1\n",
      "berle, 1\n",
      "“look, 1\n",
      "milton, 1\n",
      "only 1\n",
      "way 4\n",
      "you’ll 1\n",
      "learn 1\n",
      "sing 2\n",
      "listen 1\n",
      "billie 1\n",
      "holiday 1\n",
      "find 1\n",
      "out 3\n",
      "how 1\n",
      "play 1\n",
      "note 1\n",
      "lord 1\n",
      "given 1\n",
      "voice; 1\n",
      "mother 1\n",
      "wit 1\n",
      "magpie’s 1\n",
      "cunning 1\n",
      "account 1\n",
      "enduring 1\n",
      "distinction 1\n",
      "rest 1\n",
      "our 2\n",
      "relations 1\n",
      "were 1\n",
      "always 1\n",
      "cordial, 1\n",
      "however 1\n",
      "fleeting, 1\n",
      "but 1\n",
      "it 27\n",
      "didn’t 1\n",
      "take 1\n",
      "too 1\n",
      "long 2\n",
      "recognize 1\n",
      "puritanical 1\n",
      "boy 1\n",
      "beneath 1\n",
      "skin 1\n",
      "once 1\n",
      "passed 1\n",
      "few 1\n",
      "minutes 1\n",
      "after 1\n",
      "rally 1\n",
      "los 1\n",
      "angeles 1\n",
      "full 1\n",
      "fit 2\n",
      "enchainment 1\n",
      "clan’s 1\n",
      "thrall, 1\n",
      "said 1\n",
      "they 1\n",
      "were: 1\n",
      "“jack, 1\n",
      "jackie, 1\n",
      "ambassador, 1\n",
      "bobby, 1\n",
      "them 1\n",
      "confess, 1\n",
      "same, 1\n",
      "coming 1\n",
      "doubt 1\n",
      "peter 1\n",
      "lawford, 1\n",
      "brother-in-law, 1\n",
      "moral 1\n",
      "fiber 1\n",
      "befitting 1\n",
      "grand 1\n",
      "connections 1\n",
      "do 3\n",
      "mean 1\n",
      "say,” 1\n",
      "i, 1\n",
      "puzzled, 1\n",
      "asked, 1\n",
      "“that, 1\n",
      "sammy 1\n",
      "davis 1\n",
      "throw 1\n",
      "wild 1\n",
      "night, 1\n",
      "lawford 1\n",
      "comes 1\n",
      "along? 1\n",
      "yes,” 1\n",
      "replied 1\n",
      "what 1\n",
      "did, 1\n",
      "because 1\n",
      "bottom 1\n",
      "believed—and 1\n",
      "could 1\n",
      "rise 1\n",
      "morning 1\n",
      "believe 1\n",
      "again—that 1\n",
      "love 1\n",
      "eternal 1\n",
      "fidelity 1\n",
      "sacred 1\n",
      "trust 2\n",
      "secret 1\n",
      "would 1\n",
      "don 1\n",
      "giovanni 1\n",
      "except 1\n",
      "merely 1\n",
      "coarse 1\n",
      "bouffe 1\n",
      "not 3\n",
      "so 4\n",
      "unvaryingly 1\n",
      "persuade 1\n",
      "himself 1\n",
      "each 1\n",
      "fresh 1\n",
      "object 1\n",
      "trifling 1\n",
      "fancy 1\n",
      "lifetime’s 1\n",
      "love? 1\n",
      "no, 1\n",
      "want 1\n",
      "know 1\n",
      "why 1\n",
      "will 1\n",
      "intrude 1\n",
      "bloodstream 1\n",
      "memories 1\n",
      "survive 1\n",
      "hundred 1\n",
      "five, 1\n",
      "forget 1\n",
      "grown 1\n",
      "man 2\n",
      "never 1\n",
      "became 1\n",
      "look 3\n",
      "quite 1\n",
      "stop 1\n",
      "being, 1\n",
      "who 1\n",
      "goes 1\n",
      "about 1\n",
      "searching 1\n",
      "singing 1\n",
      "last 1\n",
      "whitney 1\n",
      "balliett 1\n",
      "sinatra’s 1\n",
      "careers—as 1\n",
      "natural 1\n",
      "actor, 1\n",
      "tough 1\n",
      "guy, 1\n",
      "mafia 1\n",
      "friend, 1\n",
      "womanizer, 1\n",
      "nouveau 1\n",
      "riche, 1\n",
      "political 1\n",
      "chameleon—that 1\n",
      "really 1\n",
      "mattered 1\n",
      "other 4\n",
      "largely 1\n",
      "career 1\n",
      "samaritan 1\n",
      "ready 2\n",
      "friends—say 1\n",
      "singer 1\n",
      "sylvia 1\n",
      "syms, 1\n",
      "or 28\n",
      "composer 1\n",
      "alec 1\n",
      "wilder—fell 1\n",
      "ill 5\n",
      "trouble 1\n",
      "microphone 1\n",
      "voice, 1\n",
      "baritone 1\n",
      "barely 1\n",
      "two 1\n",
      "octaves 1\n",
      "slightly 1\n",
      "quavery 1\n",
      "twenties 1\n",
      "vibrato 1\n",
      "learned 1\n",
      "control 1\n",
      "almost 1\n",
      "immediately 1\n",
      "kind 1\n",
      "open 1\n",
      "density, 1\n",
      "hoboken 1\n",
      "bel 2\n",
      "canto 1\n",
      "quality; 1\n",
      "despite 1\n",
      "host 1\n",
      "imitators, 1\n",
      "invariably 1\n",
      "measure 1\n",
      "sometimes 1\n",
      "baggy 1\n",
      "emotional 1\n",
      "range, 1\n",
      "easy 1\n",
      "swing, 1\n",
      "shapely 1\n",
      "dynamics, 1\n",
      "extraordinary 1\n",
      "enunciation 1\n",
      "(try 1\n",
      "garbled 1\n",
      "word 1\n",
      "lyric) 1\n",
      "directly 1\n",
      "bing 1\n",
      "crosby 1\n",
      "and, 1\n",
      "later 1\n",
      "life, 1\n",
      "disparate 1\n",
      "masters—his 1\n",
      "old 1\n",
      "boss, 1\n",
      "trombonist 1\n",
      "bandleader 1\n",
      "tommy 1\n",
      "dorsey, 1\n",
      "whose 1\n",
      "notes 1\n",
      "hung 1\n",
      "air 2\n",
      "like 1\n",
      "gliders; 1\n",
      "cabaret 1\n",
      "mabel 1\n",
      "mercer, 1\n",
      "three-dimensional, 1\n",
      "mother-hen 1\n",
      "phrasing 1\n",
      "writer 1\n",
      "lyricist 1\n",
      "gene 1\n",
      "lees 1\n",
      "has 1\n",
      "called 1\n",
      "“poet 1\n",
      "laureate 1\n",
      "good, 1\n",
      "write 1\n",
      "songs; 1\n",
      "got 2\n",
      "inside 1\n",
      "them, 1\n",
      "found 1\n",
      "their 1\n",
      "poetry, 1\n",
      "gave 1\n",
      "best 1\n",
      "indivisible 1\n",
      "parts: 1\n",
      "cocky, 1\n",
      "supreme 1\n",
      "passion 1\n",
      "songs 1\n",
      "kern 1\n",
      "gershwin 1\n",
      "arlen 1\n",
      "berlin 1\n",
      "rodgers 1\n",
      "porter 1\n",
      "helped 1\n",
      "turn 1\n",
      "american 1\n",
      "lieder 1\n"
     ]
    }
   ],
   "source": [
    "#Establish wordDict\n",
    "wordDict = {}\n",
    "for w in range(len(wordList)):\n",
    "    newWord = wordList[w]\n",
    "    newWord = newWord.lower()\n",
    "    newWord = newWord.replace('.', '')\n",
    "    wordDict[w] = newWord\n",
    "#print(wordDict)\n",
    "    \n",
    "#Perform word counts on dict\n",
    "countDict = {}\n",
    "for x in range(len(wordDict)):\n",
    "    term = wordDict[x]\n",
    "    count = 1\n",
    "    for y in range(len(wordDict)):\n",
    "        try:\n",
    "            if wordDict[y].find(term) > 0:\n",
    "                count += 1\n",
    "        except:\n",
    "            pass\n",
    "        countDict[term] = count\n",
    "\n",
    "for k, v in countDict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computes ratio of word's appearances to total words\n",
    "bow = wordList\n",
    "bowCount = len(bow) #BOW = Bag of Words\n",
    "tfDict = {}\n",
    "for term, count in countDict.items():\n",
    "    tfDict[term] = count/float(bowCount)\n",
    "\n",
    "num = dict(sorted(tfDict.items(), key=lambda x: x[1], reverse = True))\n",
    "#for (k, v in num.items()):\n",
    "    #print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORM A PHRASE FREQUENCY COUNT\n",
    "\n",
    "Now we can identify common phrases by performing a frequency count on each phrase.  Moreover, if the corpus is large enough, commonly used phrases will be evident with higher counts across many texts.  For this reason the phrase list along with counts, will be stored in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase frequency count\n",
    "from operator import itemgetter\n",
    "wordfreq = []\n",
    "for u in range(len(phrases)):\n",
    "    utterance = phrases[u]\n",
    "    uttcnt = 0\n",
    "    uttcnt = phrases.count(utterance)\n",
    "    if uttcnt > 1:\n",
    "        wordfreq.append(uttcnt)\n",
    "    \n",
    "zipped = list(zip(phrases, wordfreq))\n",
    "sortzip = sorted(zipped, key=itemgetter(1), reverse=True)\n",
    "phraseFreqDict = dict(sortzip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-54488a4c26c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#START HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mphraseFreqDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mnewPhrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphraseFreqDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewPhrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# First Hash the Phrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "If a sentence is usually 'subject' then 'verb' and then 'noun', we could assume that a good sentence or \n",
    "phrase (which could or also could not be a sentence), would have that structure. Simply, the common research topics \n",
    "of 'attention' 'sentiment' and 'semantics' match these structural terms in functionality, at least somewhat. \n",
    "We use a trifecta of unit vectors initially therefore to represent each of these terms, and apply them in a relatively \n",
    "intuitive fashion. Semantics are often considered ordinal and therefore a vector in 3-space could be (1,1,0). Taking a \n",
    "cartesian approach to the 3-space, Sentiment could be described as (1,0,1) when flat and Attention has been described \n",
    "as nearly orthogonal to sentiment and could therefore be (0,1,1).\n",
    "\"\"\"\n",
    "\n",
    "import xxhash, numpy\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "phraseDictH = {}\n",
    "# Estimate the size of the vocabulary\n",
    "vocabSize = len(phrases)\n",
    "docSize = len(spoken)\n",
    "phraseFreqs = phraseFreqDict.values()\n",
    "phraseVocab = phraseFreqDict.keys() \n",
    "\n",
    "# Return a dictionary whose keys are phrase tokens, the values are the indices       \n",
    "#START HERE\n",
    "for key,val in phraseFreqDict.items():\n",
    "    newPhrase = phraseFreqDict[val]\n",
    "    print(newPhrase)\n",
    "    # First Hash the Phrases\n",
    "    hashedPhrase = xxhash.xxh32(newPhrase, seed=60155748).hexdigest()\n",
    "    # Second, Pad the Hashes if they are NOT in hex (this is worth trying since hex is not ideal for text)\n",
    "    # Third, Vectorize the Padded or hex Hashes, using an embedding vector set from attention, semantics, \n",
    "    # or sentiment\n",
    "    phraseDictH[z] = hashedPhrase\n",
    "    \n",
    "# Last, One-Hot the Vectors\n",
    "hashSize = len(hashedPhrase)\n",
    "\n",
    "print(sortzip[ v(0) ])\n",
    "\n",
    "array = numpy.zeros( (docSize, hashSize, vocabSize), dtype=float, order='F')\n",
    "for i in range(26): #This is the Z-axis    \n",
    "    for j in enumerate(phraseDictH):\n",
    "        array[i, j, phraseDictH[phraseDictH[z]]] = 1 #Here, the third param must be the IDF?\n",
    "\n",
    "result = encoder_input_data\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the encoding\n",
    "The TF-IDF and One-Hot encoder, should produce nearly the same output.  One-Hot produces a Gaussian Intensity for a phrase amongst the list of phrases, while the Inverse Document Frequency should do similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO BE CONTINUED...\n",
    "At this point, this notebook is finished. Please refer to Part 2 for a continuation of this process, wherein the code in this notebook is converted into objects and a multiple document corpus is compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
