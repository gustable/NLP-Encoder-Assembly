{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETTING AND LOADING A CORPUS\n",
    "\n",
    "https://nlpforhackers.io/corpora/\n",
    "http://lucumr.pocoo.org/2015/11/18/pythons-hidden-re-gems/\n",
    "\n",
    "The goal of this step is to develop an initial list of each character and their spoken lines. Dictionaries are Hash value arbitrary, so may not be ordered the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Select and Read a file into \"f\" using a list and stripping out all Project Gutenberg headers and footers\n",
    "from pathlib import Path\n",
    "import re\n",
    "idx = 1\n",
    "data_folder = Path(\"data/shakespeare/\")\n",
    "file_to_open = data_folder / \"1526.txt\"\n",
    "f = open(file_to_open, 'r')\n",
    "first_document = list(f)\n",
    "\n",
    "for first_header_index in range( len(first_document) ):\n",
    "    if ( first_document[first_header_index].startswith('a team of about twenty Project Gutenberg volunteers.\\n') ) :\n",
    "           break\n",
    "            \n",
    "second_document = list(first_document[first_header_index + 1 :])\n",
    "\n",
    "for second_header_index in range( len(second_document) ):\n",
    "    if ( second_document[second_header_index].startswith('a team of about twenty Project Gutenberg volunteers.\\n') ) :\n",
    "           break            \n",
    "for footer_index in range( len(first_document) ):\n",
    "    if ( first_document[footer_index].startswith('End of Project Gutenberg Etext of') ) :\n",
    "           break\n",
    "            \n",
    "            \n",
    "script = list()\n",
    "script = list(first_document[first_header_index + second_header_index + 2 : footer_index-1])\n",
    "\n",
    "#print(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile a list of speakers\n",
    "r = re.compile(\"[A-Z0-9][A-Z0-9]+\")\n",
    "speakers = []\n",
    "for line in script:\n",
    "    mtch = r.match(line)\n",
    "    if mtch:\n",
    "        speakers.append(mtch.group())\n",
    "#print(speakers)\n",
    "\n",
    "#Omit speakers from the list of text\n",
    "s = re.compile(r\"\\b[A-Z{3}\\.]+\\b\")\n",
    "spoken = list(filter(lambda i: not s.search(i), script))\n",
    "\n",
    "#print(spoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Concatenate all lines into a single list entry for future sentence splitting\n",
    "singleLine = ''\n",
    "\n",
    "#split (at line breaks) into new list(s)\n",
    "for i in range(0, 30, 1):\n",
    "    g = spoken[i]\n",
    "    singleLine = singleLine.join(g)\n",
    "print(singleLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Error at 2890 4\n",
      "Index Error at 2892 2\n"
     ]
    }
   ],
   "source": [
    "#Concatenate lines into list entries for future sentence splitting\n",
    "\n",
    "newLines = []\n",
    "singleLine = ''\n",
    "sentencer = []\n",
    "\n",
    "#Remove all line returns(ok)\n",
    "for j in range(0, len(spoken)):\n",
    "    spoken[j] = spoken[j].replace('\\n', '')\n",
    "    \n",
    "#Split 5 lines at a time into new list\n",
    "for k in range( 0, len(spoken), 2):\n",
    "    newLines = []\n",
    "    for line in range( 0, 5 ):\n",
    "        try:\n",
    "            newLines.append(' '+spoken[line+k])\n",
    "        except:\n",
    "            print(\"Index Error at\", k, line)\n",
    "            break\n",
    "    #Join 5-line groups into one line and append to a list\n",
    "    singleLine = ''.join(newLines)\n",
    "    sentencer.append(singleLine)\n",
    "#print(sentencer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of sentences\n",
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of phrases\n",
    "phrases = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####Load sentencized text into list, sentence by sentence\n",
    "s = \"\\n\"\n",
    "for line in spoken:\n",
    "    spoken.split(\"\\n\")\n",
    "    mtch = s.match(line)\n",
    "    if mtch:\n",
    "        speakers.append(mtch.group())\n",
    "= re.compile(\"(?<!^)\\s+(?=[A-Z])(?!.\\s)\").split(spoken)\n",
    "#slines = spoken.split(\"\\n\")\n",
    "print(spoken)\n",
    "\n",
    "for i in range(0, len(lines), 1):\n",
    "    lineNo = i\n",
    "    words = lines[i]\n",
    "    \n",
    "#dictionary = dict(zip(speakers, spoken))\n",
    "#print(dictionary)\n",
    "\n",
    "#keyedDict = {}\n",
    "#keyedDict = {\"character\" : speakers, \"words\" : spoken}\n",
    "#print(keyedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "freqdict = defaultdict(int)\n",
    "for key, value in dictionary.items():\n",
    "    utterance = dictionary[spoken(key)]\n",
    "    cleantext = re.sub(r'\\n', '', utterance) #.strip().split() # Remove HTML tags and split the review by word (space separated)\n",
    "    for word in cleantext:\n",
    "        # Convert to all lowercase\n",
    "        word = word.lower()\n",
    "    \n",
    "        # Complete the following line to increase the count by one:\n",
    "        freqdict[word] += 1\n",
    "    \n",
    "print(freqdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "https://cdn-images-1.medium.com/max/800/1*nSqHXwOIJ2fa_EFLTh5KYw.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes ratio of word's appearances to total words\n",
    "def computeTF(wordDict, bow): \n",
    "    bowCount = len(bow) #BOW = Bag of Words\n",
    "    tfDict = {}\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the weight of rare words across all docs\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "                \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "    \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
