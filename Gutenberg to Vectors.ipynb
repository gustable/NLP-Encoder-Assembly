{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # From the previous notebook, \"Phrase Sampling (Part 1)\" we assemble a collection of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main\n",
    "    select docs for corpus from a list\n",
    "    execute the pull\n",
    "    execute each step in the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re, os, math\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDocs(author1, *authors2):\n",
    "    #load a selection of texts by selected authors\n",
    "    auths = [author1]\n",
    "    re1 = re.compile('(\\w+)')\n",
    "    for other_author in authors2:\n",
    "        a1 = str(other_author)\n",
    "        match = re1.search(a1)\n",
    "        if match:\n",
    "            auths.append(match.group())\n",
    "    docs = {}\n",
    "    for author in auths:\n",
    "        #print(author)\n",
    "        data_folder = Path(\"data/\" + author)\n",
    "        idx = 0\n",
    "        for file in data_folder.iterdir():\n",
    "            if str(file).endswith(\".txt\"):\n",
    "                file_to_open = file\n",
    "                o = open(file_to_open, 'r')\n",
    "                documentName = idx\n",
    "                idx += 1\n",
    "                document = list(o)\n",
    "                docs[documentName] = document\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimHeaders(first_document):\n",
    "    # Determine whether a Project Gutenberg Text\n",
    "    first_header_index = 0\n",
    "    second_header_index = 0\n",
    "    footer_index = 0\n",
    "    if any(\"GUTENBERG\" in s for s in first_document):\n",
    "        for first_header_index in range( len(first_document) ):\n",
    "            if ( ( first_document[first_header_index].find('*END*THE SMALL PRINT!') ) != -1 ) :\n",
    "                break\n",
    "            else:\n",
    "                for first_header_index in range( len(first_document) ):\n",
    "                    if ( ( first_document[first_header_index].find('START OF THIS PROJECT GUTENBERG') ) != -1 ) :\n",
    "                        break        \n",
    "        second_document = list(first_document[first_header_index + 1 :])\n",
    "        for second_header_index in range( len(second_document) ):\n",
    "            if ( ( second_document[second_header_index].find('www.gutenberg.org') ) != -1 ) :\n",
    "                break            \n",
    "        for footer_index in range( len(first_document) ):\n",
    "            if ( ( first_document[footer_index].find('End of Project') ) != -1 ) :\n",
    "                break\n",
    "            else:\n",
    "                for footer_index in range( len(first_document) ):\n",
    "                    if ( ( first_document[footer_index].find('End of the Project') ) != -1 ) :\n",
    "                        break    \n",
    "        script = list()\n",
    "        if (second_header_index < (first_header_index + 100)):\n",
    "            manuscript = list(first_document[first_header_index +1 + second_header_index +1 : footer_index-1])\n",
    "        else:\n",
    "            manuscript = list(first_document[first_header_index +1 : footer_index-1])\n",
    "    else:\n",
    "        manuscript = first_document\n",
    "    return manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectLines(script):\n",
    "    #Compile a list of speakers\n",
    "    r = re.compile(\"[A-Z0-9][A-Z0-9]+\")\n",
    "    speakers = []\n",
    "    for line in script:\n",
    "        mtch = r.match(line)\n",
    "        if mtch:\n",
    "            speakers.append(mtch.group())\n",
    "    #Omit speakers from the list of text\n",
    "    s = re.compile(r\"\\b[A-Z{3}\\.]+\\b\")\n",
    "    spoken = list(filter(lambda i: not s.search(i), script))\n",
    "    return speakers, spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencer(spoken):\n",
    "    #Concatenate lines into list entries for future sentence splitting\n",
    "    newLines = []\n",
    "    singleLine = ''\n",
    "    singleLines = []\n",
    "\n",
    "    #Remove all line returns(ok)\n",
    "    for j in range(0, len(spoken)):\n",
    "        spoken[j] = spoken[j].replace('\\n', '')\n",
    "\n",
    "    #Split 5 lines at a time into new list\n",
    "    for k in range( 0, len(spoken), 3):\n",
    "        newLines = []\n",
    "        for line in range( 0, 3 ):\n",
    "            try:\n",
    "                newLines.append(' '+spoken[line+k])\n",
    "            except:\n",
    "                #print(\"Index Error at\", k, line)\n",
    "                break\n",
    "        #Join 5-line groups into one line and append to a list\n",
    "        singleLine = ''.join(newLines)\n",
    "        singleLines.append(singleLine)\n",
    "    \n",
    "    #Create list of sentences\n",
    "    sentences = []\n",
    "    for m in range(0, len(singleLines)):\n",
    "        mtch = re.findall(\"[A-Z][^\\.!?]*[\\.!?]\", singleLines[m], re.M|re.I)\n",
    "        if mtch:\n",
    "            sentences.append(mtch)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStopList():\n",
    "    #Clean the stopword list\n",
    "    stoplist = []\n",
    "    clean_line = []\n",
    "    data_folder = Path(\"data/\")\n",
    "    file_to_open = data_folder / \"snowball_stop.txt\"\n",
    "    f = open(file_to_open, 'r')\n",
    "    full_stop = list(f)\n",
    "\n",
    "    for n in range( 0, len(full_stop), 1 ):\n",
    "        clean_line = full_stop[n].split('|')\n",
    "        clean_line[0] = clean_line[0].replace(' ', '')\n",
    "        stoplist.append(clean_line[0])\n",
    "\n",
    "    for p in range(len(stoplist)):\n",
    "        stoplist[p] = stoplist[p].replace('\\n', '')\n",
    "\n",
    "    #print(stoplist)\n",
    "    return stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectPhrases(sentences, stoplist):\n",
    "    # Create list of phrases using stopwords\n",
    "    phrases = []\n",
    "    candidate_phrases = []\n",
    "\n",
    "    for q in range(len(sentences)):\n",
    "        for r in sentences[q]:\n",
    "            words = re.split(\"\\\\s+\", r)\n",
    "            previous_stop = False\n",
    "\n",
    "            # Examine each word to determine if it is a phrase boundary marker or part of a phrase or alone\n",
    "            for w in words:\n",
    "\n",
    "                if w in stoplist and not previous_stop:\n",
    "                    # phrase boundary encountered, so put a hard indicator\n",
    "                    candidate_phrases.append(\";\")\n",
    "                    previous_stop = True\n",
    "                elif w not in stoplist and len(w) > 3:\n",
    "                    # keep adding words to list until a phrase boundary is detected\n",
    "                    candidate_phrases.append(w.strip())\n",
    "                    previous_stop = False\n",
    "\n",
    "        # Create a list of candidate phrases without boundary demarcation\n",
    "        phrases = re.split(\";+\", ' '.join(candidate_phrases))\n",
    "\n",
    "    # Clean up phrases    \n",
    "    re2 = re.compile('[^\\.!?,\"(){}\\*:]*[\\.!?,\"(){}\\*:]')\n",
    "    for s in range(len(phrases)):\n",
    "        phrases[s] = re.sub(re2, '', phrases[s])\n",
    "        phrases[s] = phrases[s].strip(' ')\n",
    "        phrases[s] = phrases[s].replace(' ', '_')\n",
    "        phrases[s] = phrases[s].replace('__', '_')\n",
    "        phrases[s] = phrases[s].strip('_')\n",
    "\n",
    "    for s in range(len(phrases)):\n",
    "        try:\n",
    "            phrases.remove('')\n",
    "            phrases.remove(' ')\n",
    "            phrases.remove('/n')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #for t in range(len(phrases)):\n",
    "        #print(phrases[t])\n",
    "    \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phraseFreq(phrases):\n",
    "    # Phrase frequency count\n",
    "    wordfreq = []\n",
    "    for u in range(len(phrases)):\n",
    "        utterance = phrases[u]\n",
    "        uttcnt = 0\n",
    "        uttcnt = phrases.count(utterance)\n",
    "        if uttcnt > 1:\n",
    "            wordfreq.append(uttcnt)\n",
    "    zipped = list(zip(phrases, wordfreq))\n",
    "    sortzip = sorted(zipped, key=itemgetter(1), reverse=True)\n",
    "    #for v in range(len(sortzip)):\n",
    "        #print(sortzip[v])  \n",
    "    return sortzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectWords(sentences):\n",
    "    #Establish wordList\n",
    "    wordList = []\n",
    "    for u in range(len(sentences)):\n",
    "        for v in sentences[u]:\n",
    "            words = re.split(\"\\\\s+\", v)\n",
    "            wordList.extend(words)\n",
    "    #Establish wordDict\n",
    "    wordDict = {}\n",
    "    for w in range(len(wordList)):\n",
    "        newWord = wordList[w]\n",
    "        newWord = newWord.lower()\n",
    "        newWord = newWord.replace('.', '')\n",
    "        wordDict[w] = newWord\n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFreq(wordDict):\n",
    "    #Perform word counts on dict\n",
    "    countDict = {}\n",
    "    for x in range(len(wordDict)):\n",
    "        term = wordDict[x]\n",
    "        #print(wordDict)\n",
    "        count = 1\n",
    "        for y in range(len(wordDict)):\n",
    "            try:\n",
    "                if wordDict[y].find(term) > 0:\n",
    "                    count += 1\n",
    "            except:\n",
    "                pass\n",
    "            countDict[term] = count #MAJOR ERROR HERE \"TypeError: unhashable type: 'dict'\"\n",
    "\n",
    "    #for k, v in countDict.items():\n",
    "        #print(k, v)\n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    # Calculates the weight of rare words across all docs\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Skip-gram Encoding of Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "documents = loadDocs('poe')\n",
    "#print(documents)\n",
    "# wordsPulled = []\n",
    "tf = []\n",
    "for d in range(len(documents)):\n",
    "    trimmedDoc = trimHeaders(documents[d])\n",
    "    heads, linesCollected = collectLines(trimmedDoc)\n",
    "    sentencesPulled = sentencer(linesCollected)\n",
    "    stoplist = readStopList()\n",
    "    phrasesPulled = collectPhrases(sentencesPulled, stoplist)\n",
    "    phraseFreq(phrasesPulled)\n",
    "    a = collectWords(sentencesPulled)\n",
    "#     wordsPulled.append(a) \n",
    "    b = wordFreq(a)\n",
    "    tf.append(b)\n",
    "idf = computeIDF(documents)\n",
    "tfidf = []\n",
    "for doc in documents:\n",
    "    c = computeTFIDF(wordsPulled(doc), idf)\n",
    "    tfidf.append(c)\n",
    "#print(tfidf)\n",
    "# Define the document\n",
    "text = tfidf[0]\n",
    "print(text)\n",
    "# Estimate the size of the vocabulary\n",
    "#words = set(text_to_word_sequence(text))\n",
    "#vocab_size = len(words)\n",
    "#print(vocab_size)\n",
    "# Integer encode the document\n",
    "#result = one_hot(text, round(vocab_size*1.3))\n",
    "#print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
