{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the previous notebook, \"Phrase Sampling (Part 1)\" we assemble a collection of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullDocs(paths):\n",
    "    # Select and Read a file into \"f\" using a list and stripping out all Project Gutenberg headers and footers\n",
    "    from pathlib import Path\n",
    "    import re\n",
    "    idx = 1\n",
    "    data_folder = Path(\"data/shakespeare/\")\n",
    "    file_to_open = data_folder / \"1526.txt\"\n",
    "    f = open(file_to_open, 'r')\n",
    "    first_document = list(f)\n",
    "    #print(first_document)\n",
    "\n",
    "    # Determine whether a Project Gutenberg Text\n",
    "    first_header_index = 0\n",
    "    second_header_index = 0\n",
    "    footer_index = 0\n",
    "    if any(\"GUTENBERG\" in s for s in first_document):\n",
    "        for first_header_index in range( len(first_document) ):\n",
    "            if ( ( first_document[first_header_index].find('*END*THE SMALL PRINT!') ) != -1 ) :\n",
    "                break\n",
    "            else:\n",
    "                for first_header_index in range( len(first_document) ):\n",
    "                    if ( ( first_document[first_header_index].find('START OF THIS PROJECT GUTENBERG') ) != -1 ) :\n",
    "                        break        \n",
    "\n",
    "        second_document = list(first_document[first_header_index + 1 :])\n",
    "\n",
    "        for second_header_index in range( len(second_document) ):\n",
    "            if ( ( second_document[second_header_index].find('www.gutenberg.org') ) != -1 ) :\n",
    "                break            \n",
    "        for footer_index in range( len(first_document) ):\n",
    "            if ( ( first_document[footer_index].find('End of Project') ) != -1 ) :\n",
    "                break\n",
    "            else:\n",
    "                for footer_index in range( len(first_document) ):\n",
    "                    if ( ( first_document[footer_index].find('End of the Project') ) != -1 ) :\n",
    "                        break    \n",
    "\n",
    "        print(first_header_index)            \n",
    "        print(second_header_index)\n",
    "        print(footer_index)     \n",
    "\n",
    "        script = list()\n",
    "        if (second_header_index < (first_header_index + 100)):\n",
    "            script = list(first_document[first_header_index +1 + second_header_index +1 : footer_index-1])\n",
    "        else:\n",
    "            script = list(first_document[first_header_index +1 : footer_index-1])\n",
    "    else:\n",
    "        script = first_document\n",
    "    print(script)\n",
    "    return script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectLines(script)\n",
    "    #Compile a list of speakers\n",
    "    r = re.compile(\"[A-Z0-9][A-Z0-9]+\")\n",
    "    speakers = []\n",
    "    for line in script:\n",
    "        mtch = r.match(line)\n",
    "        if mtch:\n",
    "            speakers.append(mtch.group())\n",
    "    #print(speakers)\n",
    "\n",
    "    #Omit speakers from the list of text\n",
    "    s = re.compile(r\"\\b[A-Z{3}\\.]+\\b\")\n",
    "    spoken = list(filter(lambda i: not s.search(i), script))\n",
    "\n",
    "    #print(spoken)\n",
    "    return speakers, spoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencer(spoken)\n",
    "    #Concatenate lines into list entries for future sentence splitting\n",
    "    newLines = []\n",
    "    singleLine = ''\n",
    "    singleLines = []\n",
    "\n",
    "    #Remove all line returns(ok)\n",
    "    for j in range(0, len(spoken)):\n",
    "        spoken[j] = spoken[j].replace('\\n', '')\n",
    "\n",
    "    #Split 5 lines at a time into new list\n",
    "    for k in range( 0, len(spoken), 3):\n",
    "        newLines = []\n",
    "        for line in range( 0, 3 ):\n",
    "            try:\n",
    "                newLines.append(' '+spoken[line+k])\n",
    "            except:\n",
    "                #print(\"Index Error at\", k, line)\n",
    "                break\n",
    "        #Join 5-line groups into one line and append to a list\n",
    "        singleLine = ''.join(newLines)\n",
    "        singleLines.append(singleLine)\n",
    "    \n",
    "    #Create list of sentences\n",
    "    sentences = []\n",
    "    for m in range(0, len(singleLines)):\n",
    "        mtch = re.findall(\"[A-Z][^\\.!?]*[\\.!?]\", singleLines[m], re.M|re.I)\n",
    "        if mtch:\n",
    "            sentences.append(mtch)\n",
    "    #print(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readStopList()\n",
    "    #Clean the stopword list\n",
    "    stoplist = []\n",
    "    clean_line = []\n",
    "    data_folder = Path(\"data/\")\n",
    "    file_to_open = data_folder / \"snowball_stop.txt\"\n",
    "    f = open(file_to_open, 'r')\n",
    "    full_stop = list(f)\n",
    "\n",
    "    for n in range( 0, len(full_stop), 1 ):\n",
    "        clean_line = full_stop[n].split('|')\n",
    "        stoplist.append(clean_line[0])\n",
    "\n",
    "    for p in range(len(stoplist)):\n",
    "        stoplist[p] = stoplist[p].replace('\\n', '')\n",
    "\n",
    "    #print(stoplist)\n",
    "    return stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectPhrases(sentences, stoplist)\n",
    "    # Create list of phrases using stopwords\n",
    "    phrases = []\n",
    "    candidate_phrases = []\n",
    "\n",
    "    for q in range(len(sentences)):\n",
    "        for r in sentences[q]:\n",
    "            words = re.split(\"\\\\s+\", r)\n",
    "            previous_stop = False\n",
    "\n",
    "            # Examine each word to determine if it is a phrase boundary marker or part of a phrase or alone\n",
    "            for w in words:\n",
    "\n",
    "                if w in stoplist and not previous_stop:\n",
    "                    # phrase boundary encountered, so put a hard indicator\n",
    "                    candidate_phrases.append(\";\")\n",
    "                    previous_stop = True\n",
    "                elif w not in stoplist and len(w) > 3:\n",
    "                    # keep adding words to list until a phrase boundary is detected\n",
    "                    candidate_phrases.append(w.strip())\n",
    "                    previous_stop = False\n",
    "\n",
    "        # Create a list of candidate phrases without boundary demarcation\n",
    "        phrases = re.split(\";+\", ' '.join(candidate_phrases))\n",
    "\n",
    "    # Clean up phrases    \n",
    "    re2 = re.compile('[^\\.!?,\"(){}\\*:]*[\\.!?,\"(){}\\*:]')\n",
    "    for s in range(len(phrases)):\n",
    "        phrases[s] = re.sub(re2, '', phrases[s])\n",
    "        phrases[s] = phrases[s].strip(' ')\n",
    "        phrases[s] = phrases[s].replace(' ', '_')\n",
    "        phrases[s] = phrases[s].replace('__', '_')\n",
    "        phrases[s] = phrases[s].strip('_')\n",
    "\n",
    "    for s in range(len(phrases)):\n",
    "        try:\n",
    "            phrases.remove('')\n",
    "            phrases.remove(' ')\n",
    "            phrases.remove('/n')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for t in range(50):\n",
    "        print(phrases[t])\n",
    "    \n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phraseFreq(phrases)    \n",
    "    # Phrase frequency count\n",
    "    from operator import itemgetter\n",
    "    wordfreq = []\n",
    "    for u in range(len(phrases)):\n",
    "        utterance = phrases[u]\n",
    "        uttcnt = 0\n",
    "        uttcnt = phrases.count(utterance)\n",
    "        if uttcnt > 1:\n",
    "            wordfreq.append(uttcnt)\n",
    "\n",
    "    zipped = list(zip(phrases, wordfreq))\n",
    "    sortzip = sorted(zipped, key=itemgetter(1), reverse=True)\n",
    "\n",
    "    for v in range(50):\n",
    "        print(sortzip[v])\n",
    "        \n",
    "    return sortzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordFreq(sentences)\n",
    "    #Establish wordList\n",
    "    wordList = []\n",
    "    for u in range(len(sentences)):\n",
    "        for v in sentences[u]:\n",
    "            words = re.split(\"\\\\s+\", v)\n",
    "            wordList.extend(words)\n",
    "\n",
    "    #Establish wordDict\n",
    "    wordDict = {}\n",
    "    for w in range(len(wordList)):\n",
    "        newWord = wordList[w]\n",
    "        newWord = newWord.lower()\n",
    "        newWord = newWord.replace('.', '')\n",
    "        wordDict[w] = newWord\n",
    "    #print(wordDict)\n",
    "\n",
    "    #Perform word counts on dict\n",
    "    countDict = {}\n",
    "    for x in range(len(wordDict)):\n",
    "        term = wordDict[x]\n",
    "        count = 1\n",
    "        for y in range(len(wordDict)):\n",
    "            try:\n",
    "                if wordDict[y].find(term) > 0:\n",
    "                    count += 1\n",
    "            except:\n",
    "                pass\n",
    "            countDict[term] = count\n",
    "\n",
    "    #for k, v in countDict.items():\n",
    "        #print(k, v)\n",
    "    return countDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList)\n",
    "    # Calculates the weight of rare words across all docs\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
