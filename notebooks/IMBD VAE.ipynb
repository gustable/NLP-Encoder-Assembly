{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoder (VAE) \n",
    " (inspired by the work of @AlexAdam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras import objectives, backend as K\n",
    "from keras.layers import Input, Dense, Embedding, Bidirectional, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import vae\n",
    "\n",
    "\n",
    "class Hyper(vae.Hyper):\n",
    "    def __init__(self,\n",
    "            vocab_size=1000, \n",
    "            embedding_dim=64, \n",
    "            max_length=300,\n",
    "            batch_size=10,\n",
    "            lr=0.001, \n",
    "            latent_dim=435,\n",
    "            intermediate_dim=200,\n",
    "            encoder_hidden_dim=500, \n",
    "            decoder_hidden_dim=500,\n",
    "            epochs=50,\n",
    "            epsilon_std=0.01):\n",
    "        vae.Hyper.__init__(self,            \n",
    "            batch_size=batch_size,\n",
    "            lr=lr, \n",
    "            latent_dim=latent_dim,\n",
    "            intermediate_dim=intermediate_dim,\n",
    "            epochs=epochs,\n",
    "            epsilon_std=epsilon_std)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_length = max_length\n",
    "        self.encoder_hidden_dim=encoder_hidden_dim\n",
    "        self.decoder_hidden_dim=decoder_hidden_dim\n",
    "\n",
    "\n",
    "class TextVae(vae.Vae):\n",
    "    def __init__(self, hyper):\n",
    "        vae.Vae.__init__(self, hyper)\n",
    "\n",
    "    def build_optimizer(self):\n",
    "        return Adam(lr=self.h.lr)\n",
    "        \n",
    "    def compute_vae_loss(self, x, x_decoded_mean, z_mean, z_log_var):\n",
    "        print(\"shapes\")\n",
    "        print(x.shape)\n",
    "        print(x.dtype)\n",
    "        print(x_decoded_mean.shape)\n",
    "        x = K.flatten(x)\n",
    "#         x = K.one_hot(x, self.h.vocab_size)\n",
    "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
    "        xent_loss = self.h.max_length * objectives.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "#     def build_vae_loss_layer(self, x, x_decoded_mean, z_mean, z_log_var):\n",
    "#         return vae.Vae.build_vae_loss_layer(self, self.x_embed, x_decoded_mean, z_mean, z_log_var)\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        x = Input(shape=(self.h.max_length,))\n",
    "        self.x_embed = Embedding(self.h.vocab_size, self.h.embedding_dim, input_length=self.h.max_length)(x)\n",
    "        h = Bidirectional(LSTM(self.h.encoder_hidden_dim, \n",
    "                return_sequences=True, \n",
    "                name='encoder_rnn_1'), \n",
    "            merge_mode='concat')(self.x_embed)\n",
    "        h = Bidirectional(LSTM(self.h.encoder_hidden_dim, \n",
    "                return_sequences=False, \n",
    "                name='encoder_rnn_2'),\n",
    "            merge_mode='concat')(h)\n",
    "\n",
    "        return x, Dense(self.h.intermediate_dim, activation='relu', name='encoder_output')(h)\n",
    "            \n",
    "    def build_decoder_layers(self):\n",
    "        decoder_rnn_1 = LSTM(self.h.decoder_hidden_dim, \n",
    "            return_sequences=True, \n",
    "            name='decoder_rnn_1')\n",
    "        decoder_rnn_2 = LSTM(self.h.decoder_hidden_dim,\n",
    "            return_sequences=True, \n",
    "            name='decoder_rnn_2')\n",
    "        decoder_mean = TimeDistributed(Dense(self.h.vocab_size, activation='softmax'), name='decoded_mean')\n",
    "        return decoder_rnn_1, decoder_rnn_2, decoder_mean\n",
    "\n",
    "    def build_decoder(self, z):\n",
    "        h_decoded = decoder_h(z)\n",
    "        return decoder_mean(h_decoded)\n",
    "\n",
    "    def build_decoder(self, encoded):\n",
    "        decoder_rnn_1, decoder_rnn_2, decoder_mean = self.decoder_layers\n",
    "\n",
    "        h = RepeatVector(self.h.max_length)(encoded)\n",
    "        h = decoder_rnn_1(h)\n",
    "        h = decoder_rnn_2(h)\n",
    "\n",
    "        return decoder_mean(h)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        decoder_rnn_1, decoder_rnn_2, decoder_mean = self.decoder_layers\n",
    "\n",
    "        decoder_input = Input(shape=(self.h.latent_dim,))\n",
    "\n",
    "        h = RepeatVector(self.h.max_length)(decoder_input)\n",
    "        h = decoder_rnn_1(h)\n",
    "        h = decoder_rnn_2(h)\n",
    "\n",
    "        return Model(decoder_input, decoder_mean(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes\n",
      "(?, ?, ?)\n",
      "<dtype: 'float32'>\n",
      "(?, 300, 1000)\n"
     ]
    }
   ],
   "source": [
    "hyper = Hyper()\n",
    "model = TextVae(hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "NUM_WORDS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "(25000,)\n",
      "(25000,)\n",
      "Number of words:\n",
      "998\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"Number of words:\")\n",
    "print(len(np.unique(np.hstack(X_train))))\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=MAX_LENGTH)\n",
    "X_test = pad_sequences(X_test, maxlen=MAX_LENGTH)\n",
    "\n",
    "train_indices = np.random.choice(np.arange(X_train.shape[0]), 2000, replace=False)\n",
    "test_indices = np.random.choice(np.arange(X_test.shape[0]), 1000, replace=False)\n",
    "\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "X_test = X_test[test_indices]\n",
    "y_test = y_test[test_indices]\n",
    "\n",
    "temp = np.zeros((X_train.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_train.shape[0]), axis=0).reshape(X_train.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_train.shape[0], axis=0), X_train] = 1\n",
    "\n",
    "X_train_one_hot = temp\n",
    "\n",
    "temp = np.zeros((X_test.shape[0], MAX_LENGTH, NUM_WORDS))\n",
    "temp[np.expand_dims(np.arange(X_test.shape[0]), axis=0).reshape(X_test.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_test.shape[0], axis=0), X_test] = 1\n",
    "\n",
    "X_test_one_hot = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "2000/2000 [==============================] - 837s - loss: 1.3985 - val_loss: 1.3678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f319480cdd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=X_train_one_hot, batch_size=10, epochs=1, validation_data=(X_test, X_test_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = model.encode(X_test[:3])\n",
    "decode = model.generate(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   1,   6, 171,   7,  61,\n",
       "          2, 927,  28,   2,  14,   2, 114, 791,  38,  13,  80,   2,   8,\n",
       "         49,   7,   4,   2,   2,  10,  10, 300,  14,   9, 441,  21,  24,\n",
       "         18,   4,   2,   2, 104,  45, 954,  21, 820, 131,   2,  10,  10,\n",
       "        241,   2, 119, 602,  47,   8,  30,  44,   2,   4,   2, 250,  15,\n",
       "          2,   2,   9,  96,   2,  10,  10, 342,   2,   2,  27,   2,  39,\n",
       "        443,   2,  82,  48,  61,   2,   2, 272,  40,   2, 146, 170,   8,\n",
       "         28,   8,   2,  18,   6,   2,  10,  10, 470, 864,  38, 684,   2,\n",
       "          9,   2,  18,  27,   2,   2,  51, 243,   7, 527,   2,   2,   9,\n",
       "         15,   2, 560,  15,  12,  16,   2,   2,  15, 828,  27, 846, 295,\n",
       "         13,  43, 197,  15,  16, 427, 647,   2,  13,  70,   2,  18,  61,\n",
       "        846,  17,  76,  17,  13, 181,  21,   4,  64,  96,   2, 789, 295,\n",
       "          9,  48,  36,   2, 507, 170,   8,  81,  12,  10,  10, 457, 637,\n",
       "          2,   4, 795,  23,  89,  14,  20,   2, 701,   2, 684, 507,  32,\n",
       "          2,   2,  10,  10,   2,  18, 142,   2, 628, 428,   9,   6, 542,\n",
       "        284,  29, 408,   4,  20,   6, 117, 227,   7,   2,  60,  48,  29,\n",
       "          9,   4,  64,  31,  29, 184,  76,   2,  14,  22, 295,  23,  27,\n",
       "        205],\n",
       "       [780,  11,   4, 666,   2,   2,   2,   2,  17,   6, 968,   2,  23,\n",
       "          4, 314,   7,   4,   2, 500,   7,   4,   2, 201,  29, 214,   2,\n",
       "         38,  29,  70,   2,  27,   2, 476,   8,   4,   2, 500,   5,   7,\n",
       "        265,   2,   6, 117,  58,  19,  98,  82,   2,   4,   2,  21,  18,\n",
       "        993,   2,   9,   4,   2,   2,   7,   4,   2,   2,   2,   2,   2,\n",
       "         54,   6, 762,   7, 966,  73,   2,   2,   2,   4,   2,   2,   2,\n",
       "        953,   5,   2,   2,   7,   2,  11,   2, 278, 263, 632,   2,   2,\n",
       "         47,  27,   2,  53,  74,   2, 276,  46,  10,  10, 257,  58,   6,\n",
       "        162,  31,   7, 134,   2,   2,   9,  93,  12, 186,  15,  32,  75,\n",
       "         79,  11,   4,  96,   7,   2,   9,   2,  78, 493,   5,  53,   5,\n",
       "         53,   2,   5,   2, 771,  11,  63,   4,  52, 493,   2,   7,  98,\n",
       "        167,   5,  27,   2,  28, 825, 120,   4, 350,  24,  15,  14,   9,\n",
       "         35, 204,   2,  11, 269,   8,  97, 178,   2,   4,   2,   2,   4,\n",
       "        527,   2, 140,  44,   2,   4,   2,   2,   4,   2,   5,   4,   2,\n",
       "        199, 154,   5, 185,  19, 992,   2,   5,  19,   2,   2,  18,   4,\n",
       "          2,   2,   7, 257,   2,  10,  10,   2,   2,   2,  11, 958,   7,\n",
       "          2,   5,   2,   2,   5, 341,   2,   2,   4, 229,   2, 117,   7,\n",
       "          4,   2,   2, 220, 316, 334,  47,   2,   2, 972,  39,   2,   2,\n",
       "         37, 615,   2,  27, 467, 398,   2, 217,  17,   4,   2,   2, 154,\n",
       "          2,   2,  50, 126,  16,  10,  10, 225,   6, 176,   7,   2, 627,\n",
       "          5, 195,   2, 318,   2,   2,   8, 401, 259,   2,  21,  66, 198,\n",
       "         32,  14, 206, 509,  47, 206, 151,  50,  26,  49,   2, 844, 164,\n",
       "        100,  30, 446,   2, 530,  10,  10,   2,   2, 158,   2,   2,   2,\n",
       "          2],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   1,  14,   9, 869,   4, 686, 249,\n",
       "         22,  13,  28, 126, 110,  12,  47,  57,  52, 944,  33,  32,  10,\n",
       "         10,  12, 610,  17,  48,  12,  16,  93,  11,  44, 891, 234,  19,\n",
       "          4,  85,  58,   2,  19, 425,   2,  10,  10,   4, 485, 915,   2,\n",
       "         39,   2,   2,   8,   2,   2,  25,  51,  10,  10,  85,  74,  15,\n",
       "        484, 727,   5,  24,  33,  32, 221,  10,  10,  13, 317,   4, 438,\n",
       "        547,   2,  10,  10,   2,   8, 135,  13, 100,  24,  14,  22,   8,\n",
       "        259]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
