{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_params import *\n",
    "import text_encoder as te\n",
    "import text_decoder as td\n",
    "from data_set import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bowizer\n",
    "import tfidf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dvd/repos/reading-club/dependencies/nlp-encoder-assembly/bowizer.py\n"
     ]
    }
   ],
   "source": [
    "print(bowizer.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_policy_random.pkl', 'rb') as f:\n",
    "    gd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n                    ________  ________  __    __  ________        \\n                   |        \\\\|        \\\\|  \\\\  |  \\\\|        \\\\       \\n                    \\\\$$$$$$$$| $$$$$$$$| $$  | $$ \\\\$$$$$$$$       \\n                      | $$   | $$__     \\\\$$\\\\/  $$   | $$          \\n                      | $$   | $$  \\\\     >$$  $$    | $$          \\n                      | $$   | $$$$$    /  $$$$\\\\    | $$          \\n                      | $$   | $$_____ |  $$ \\\\$$\\\\   | $$          \\n                      | $$   | $$     \\\\| $$  | $$   | $$          \\n                       \\\\$$    \\\\$$$$$$$$ \\\\$$   \\\\$$    \\\\$$          \\n              __       __   ______   _______   __        _______  \\n             |  \\\\  _  |  \\\\ /      \\\\ |       \\\\ |  \\\\      |       \\\\ \\n             | $$ / \\\\ | $$|  $$$$$$\\\\| $$$$$$$\\\\| $$      | $$$$$$$\\\\\\n             | $$/  $\\\\| $$| $$  | $$| $$__| $$| $$      | $$  | $$\\n             | $$  $$$\\\\ $$| $$  | $$| $$    $$| $$      | $$  | $$\\n             | $$ $$\\\\$$\\\\$$| $$  | $$| $$$$$$$\\\\| $$      | $$  | $$\\n             | $$$$  \\\\$$$$| $$__/ $$| $$  | $$| $$_____ | $$__/ $$\\n             | $$$    \\\\$$$ \\\\$$    $$| $$  | $$| $$     \\\\| $$    $$\\n              \\\\$$      \\\\$$  \\\\$$$$$$  \\\\$$   \\\\$$ \\\\$$$$$$$$ \\\\$$$$$$$ \\n\\nIt\\'s time to explore the amazing world of TextWorld! Make it so the safe inside the cellar is locked.\\n\\n-= Cellar =-\\nYou find yourself in a cellar. A typical one. You try to gain information on your surroundings by using a technique you call \"looking.\"\\n\\nA closed safe, which looks typical, is in the corner.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob ='\\n\\n\\n                    ________  ________  __    __  ________        \\n                   |        \\\\|        \\\\|  \\\\  |  \\\\|        \\\\       \\n                    \\\\$$$$$$$$| $$$$$$$$| $$  | $$ \\\\$$$$$$$$       \\n                      | $$   | $$__     \\\\$$\\\\/  $$   | $$          \\n                      | $$   | $$  \\\\     >$$  $$    | $$          \\n                      | $$   | $$$$$    /  $$$$\\\\    | $$          \\n                      | $$   | $$_____ |  $$ \\\\$$\\\\   | $$          \\n                      | $$   | $$     \\\\| $$  | $$   | $$          \\n                       \\\\$$    \\\\$$$$$$$$ \\\\$$   \\\\$$    \\\\$$          \\n              __       __   ______   _______   __        _______  \\n             |  \\\\  _  |  \\\\ /      \\\\ |       \\\\ |  \\\\      |       \\\\ \\n             | $$ / \\\\ | $$|  $$$$$$\\\\| $$$$$$$\\\\| $$      | $$$$$$$\\\\\\n             | $$/  $\\\\| $$| $$  | $$| $$__| $$| $$      | $$  | $$\\n             | $$  $$$\\\\ $$| $$  | $$| $$    $$| $$      | $$  | $$\\n             | $$ $$\\\\$$\\\\$$| $$  | $$| $$$$$$$\\\\| $$      | $$  | $$\\n             | $$$$  \\\\$$$$| $$__/ $$| $$  | $$| $$_____ | $$__/ $$\\n             | $$$    \\\\$$$ \\\\$$    $$| $$  | $$| $$     \\\\| $$    $$\\n              \\\\$$      \\\\$$  \\\\$$$$$$  \\\\$$   \\\\$$ \\\\$$$$$$$$ \\\\$$$$$$$ \\n\\n'\n",
    "bloblen = len(blob)\n",
    "\n",
    "gd[0] = [x.replace(blob, \"\") for x in gd[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "take stapler from toolbox\n",
      "put stapler on rack\n",
      "take stapler from toolbox\n",
      "put stapler on rack\n",
      "take stapler from toolbox\n",
      "put stapler on rack\n",
      "take stapler from chair\n",
      "drop stapler\n",
      "take stapler from chair\n",
      "drop stapler\n",
      "take stapler from chair\n",
      "drop stapler\n",
      "take stapler\n",
      "drop stapler\n",
      "take stapler\n",
      "drop stapler\n",
      "take stapler\n",
      "drop stapler\n",
      "take stapler from bureau\n",
      "take stapler from bureau\n",
      "take stapler from bureau\n",
      "take stapler\n",
      "insert stapler into safe\n",
      "take stapler\n",
      "insert stapler into safe\n",
      "take stapler\n",
      "insert stapler into safe\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def get_max_len_actions(gd):\n",
    "    wordlist = []\n",
    "    for i in gd[1]:\n",
    "        #print(i.split())\n",
    "       \n",
    "        \n",
    "           \n",
    "        wordlist.append(len(i.split()))\n",
    "    return max(wordlist)\n",
    "\n",
    "#get_max_len_actions(gd[1])\n",
    "\n",
    "def get_set_of_action_words(gd):\n",
    "    words = []\n",
    "    for i in gd[1]:\n",
    "        \n",
    "        words += i.split()\n",
    "    words = set(words)\n",
    "    return len(words)\n",
    "    \n",
    "def search_actionlist(gd, term):\n",
    "    for i in gd[1]:\n",
    "        if term in i.split():\n",
    "            print(i)\n",
    "    \n",
    "\n",
    "\n",
    "print(get_set_of_action_words(gd)) \n",
    "search_actionlist(gd, 'stapler')\n",
    "print(get_max_len_actions(gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = len(gd[1])\n",
    "\n",
    "vocab_size = 300\n",
    "training_split = 0.7\n",
    "\n",
    "traininglen = int(len_data * training_split)\n",
    "\n",
    "# - use gd[1] for actions data\n",
    "training_set = gd[1][:traininglen]\n",
    "test_set = gd[1][traininglen:]\n",
    "test_set[0]\n",
    "\n",
    "#use the game states for tokens\n",
    "tm = bowizer.TokenMaker(corpus=gd[1], vocab_size= vocab_size)\n",
    "\n",
    "train_x = tm.x(training_set, 16)\n",
    "train_y = tm.y(train_x)\n",
    "\n",
    "test_x = tm.x(test_set, 16)\n",
    "test_y = tm.y(test_x)\n",
    "\n",
    "#print(tm.vocab)\n",
    "#print(gd[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26926, 16) (26926, 16, 301)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'s\",\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'a',\n",
       " 'advent',\n",
       " 'american',\n",
       " 'apple',\n",
       " 'armchair',\n",
       " 'b',\n",
       " 'backup',\n",
       " 'ball',\n",
       " 'banana',\n",
       " 'bar',\n",
       " 'basket',\n",
       " 'bats',\n",
       " 'bear',\n",
       " 'bed',\n",
       " 'beetles',\n",
       " 'bench',\n",
       " 'berry',\n",
       " 'binder',\n",
       " 'blackberry',\n",
       " 'blanket',\n",
       " 'blender',\n",
       " 'blueberry',\n",
       " 'board',\n",
       " 'book',\n",
       " 'bookshelf',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'bread',\n",
       " 'broccoli',\n",
       " 'broom',\n",
       " 'bug',\n",
       " 'bugs',\n",
       " 'bunnies',\n",
       " 'bureau',\n",
       " 'burger',\n",
       " 'butter',\n",
       " 'butterfly',\n",
       " 'cabbage',\n",
       " 'cabinet',\n",
       " 'cake',\n",
       " 'calendar',\n",
       " 'canadian',\n",
       " 'candy',\n",
       " 'cane',\n",
       " 'cantaloupe',\n",
       " 'carrot',\n",
       " 'case',\n",
       " 'cashew',\n",
       " 'cat',\n",
       " 'caterpillars',\n",
       " 'cauliflower',\n",
       " 'cd',\n",
       " 'chair',\n",
       " 'chest',\n",
       " 'chocolate',\n",
       " 'cloak',\n",
       " 'close',\n",
       " 'clove',\n",
       " 'coconut',\n",
       " 'coffee',\n",
       " 'coffer',\n",
       " 'comic',\n",
       " 'computer',\n",
       " 'controller',\n",
       " 'cookie',\n",
       " 'couch',\n",
       " 'counter',\n",
       " 'cranberry',\n",
       " 'crate',\n",
       " 'cuboid',\n",
       " 'cucumber',\n",
       " 'cup',\n",
       " 'cushion',\n",
       " 'd',\n",
       " 'day',\n",
       " 'desk',\n",
       " 'desktop',\n",
       " 'disk',\n",
       " 'dispenser',\n",
       " 'display',\n",
       " 'door',\n",
       " 'drawer',\n",
       " 'dresser',\n",
       " 'drop',\n",
       " 'durian',\n",
       " 'dvd',\n",
       " 'e',\n",
       " 'earwigs',\n",
       " 'east',\n",
       " 'eat',\n",
       " 'edition',\n",
       " 'elderberry',\n",
       " 'f',\n",
       " 'fly',\n",
       " 'folder',\n",
       " 'fondue',\n",
       " 'fork',\n",
       " 'formless',\n",
       " 'freezer',\n",
       " 'fresh',\n",
       " 'fridge',\n",
       " 'frisbee',\n",
       " 'from',\n",
       " 'fudge',\n",
       " 'g',\n",
       " 'garlic',\n",
       " 'gate',\n",
       " 'gateway',\n",
       " 'glass',\n",
       " 'glove',\n",
       " 'go',\n",
       " 'gojiberry',\n",
       " 'golf',\n",
       " 'grape',\n",
       " 'grubs',\n",
       " 'gummy',\n",
       " 'h',\n",
       " 'hat',\n",
       " 'hatch',\n",
       " 'headphones',\n",
       " 'henderson',\n",
       " 'honeydew',\n",
       " 'huckleberry',\n",
       " 'i',\n",
       " 'insect',\n",
       " 'insects',\n",
       " 'insert',\n",
       " 'into',\n",
       " 'iron',\n",
       " 'j',\n",
       " 'k',\n",
       " 'kettle',\n",
       " 'key',\n",
       " 'keyboard',\n",
       " 'keycard',\n",
       " 'kittens',\n",
       " 'kiwi',\n",
       " 'knife',\n",
       " 'ladle',\n",
       " 'lamp',\n",
       " 'lampshade',\n",
       " 'laptop',\n",
       " 'larva',\n",
       " 'latchkey',\n",
       " 'laundry',\n",
       " 'lavender',\n",
       " 'legume',\n",
       " 'licorice',\n",
       " 'lightbulb',\n",
       " 'limited',\n",
       " 'lingonberry',\n",
       " 'loaf',\n",
       " 'lock',\n",
       " 'locker',\n",
       " 'm',\n",
       " 'mantelpiece',\n",
       " 'mantle',\n",
       " 'manuscript',\n",
       " 'mat',\n",
       " 'melon',\n",
       " 'microsoft',\n",
       " 'monitor',\n",
       " 'mop',\n",
       " 'mouse',\n",
       " 'mug',\n",
       " 'n',\n",
       " 'napkin',\n",
       " 'nest',\n",
       " 'non-euclidean',\n",
       " 'north',\n",
       " 'novel',\n",
       " 'o',\n",
       " 'of',\n",
       " 'on',\n",
       " 'onion',\n",
       " 'open',\n",
       " 'p',\n",
       " 'pair',\n",
       " 'pan',\n",
       " 'pants',\n",
       " 'paper',\n",
       " 'passageway',\n",
       " 'passkey',\n",
       " 'peanut',\n",
       " 'pear',\n",
       " 'pen',\n",
       " 'pencil',\n",
       " 'pillow',\n",
       " 'pizza',\n",
       " 'plant',\n",
       " 'plate',\n",
       " 'platter',\n",
       " 'poem',\n",
       " 'portal',\n",
       " 'portmanteau',\n",
       " 'potato',\n",
       " 'printer',\n",
       " 'puppies',\n",
       " 'put',\n",
       " 'q',\n",
       " 'quote',\n",
       " 'r',\n",
       " 'rack',\n",
       " 'raspberry',\n",
       " 'recliner',\n",
       " 'rectangular',\n",
       " 'refrigerator',\n",
       " 's',\n",
       " 'safe',\n",
       " 'salad',\n",
       " 'sandwich',\n",
       " 'saucepan',\n",
       " 'scarf',\n",
       " 'scented',\n",
       " 'shadfly',\n",
       " 'shelf',\n",
       " 'shirt',\n",
       " 'shoe',\n",
       " 'shrimp',\n",
       " 'soap',\n",
       " 'sock',\n",
       " 'south',\n",
       " 'spherical',\n",
       " 'spiders',\n",
       " 'sponge',\n",
       " 'spoon',\n",
       " 'spork',\n",
       " 'stand',\n",
       " 'staple',\n",
       " 'stapler',\n",
       " 'stick',\n",
       " 'strawberry',\n",
       " 'strip',\n",
       " 'style',\n",
       " 'suitcase',\n",
       " 'synthesizer',\n",
       " 'table',\n",
       " 'tablet',\n",
       " 'take',\n",
       " 'teacup',\n",
       " 'teapot',\n",
       " 'teaspoon',\n",
       " 'tee',\n",
       " 'telephone',\n",
       " 'textbook',\n",
       " 'textworld',\n",
       " 'the',\n",
       " 'ticks',\n",
       " 'toads',\n",
       " 'toolbox',\n",
       " 'top',\n",
       " 'towel',\n",
       " 'trunk',\n",
       " 'tv',\n",
       " 'type',\n",
       " 'u',\n",
       " 'unlock',\n",
       " 'v',\n",
       " 'vacuum',\n",
       " 'vanilla',\n",
       " 'w',\n",
       " 'watermelon',\n",
       " 'west',\n",
       " 'whisk',\n",
       " 'with',\n",
       " 'workbench',\n",
       " 'worm',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lock safe with latchkey'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dont use big.txt for tokens\n",
    "#with open('../../../data/norvig_tm_3000.pkl', 'rb') as f:\n",
    "    #tm = pickle.load(f)\n",
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_h = EmbeddingHyper(tm.vocab_size + 1, 64)\n",
    "conv_h = ConvHyper(64, 6, 4)\n",
    "rnn_h = RnnHyper(512, is_lstm=False, is_bidirectional=True, return_sequences=False)\n",
    "encoder_h = te.Hyper(embed_h, [conv_h, rnn_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dernn_h = RnnHyper(512, is_lstm=False, is_bidirectional=False, return_sequences=True, unroll=True)\n",
    "dec_h = DeconvHyper(64, 6, 4)\n",
    "decoder_h = td.Hyper(tm.vocab_size + 1, [dernn_h, dec_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder_h.make_layer()\n",
    "decoder = decoder_h.make_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(training_set).shape\n",
    "\n",
    "ml = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(max_len):\n",
    "    x = Input(shape=(max_len,), name='text_input')\n",
    "    h = encoder(x)\n",
    "    h = decoder(h, max_len)\n",
    "    model = Model(x, h)\n",
    "    model.compile(optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel = make_model(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testmodel.load_weights('../models/wl_model64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_round(history=None, epochs=100):\n",
    "    if history is None:\n",
    "        initial_epoch = 0\n",
    "    else:\n",
    "        initial_epoch = len(history['loss'])\n",
    "    #train = (lines=r.choice(training_set, size=2000, replace=False), maxlen=ml,tokenmaker=tm)\n",
    "    #test = bowizer.SlicedWordData(lines=r.choice(test_set, size=400, replace=False), maxlen=ml,tokenmaker=tm)\n",
    "    \n",
    "    newhistory = testmodel.fit(x=train_x, y=train_y,\n",
    "                            epochs=, batch_size=32,\n",
    "                            validation_data=(test_x, test_y),\n",
    "                            initial_epoch=initial_epoch)\n",
    "    if history is None:\n",
    "        history = newhistory.history\n",
    "    else:\n",
    "        history = {key:history[key] + newhistory.history[key] for key in history.keys()}\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 1/5\n",
      "26926/26926 [==============================] - 43s - loss: 2.9658 - categorical_accuracy: 0.8166 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 2/5\n",
      "26926/26926 [==============================] - 43s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 3/5\n",
      "26926/26926 [==============================] - 39s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 4/5\n",
      "26926/26926 [==============================] - 39s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 5/5\n",
      "26926/26926 [==============================] - 41s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "1\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 6/10\n",
      "26926/26926 [==============================] - 43s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 7/10\n",
      "26926/26926 [==============================] - 41s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 8/10\n",
      "26926/26926 [==============================] - 44s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 9/10\n",
      "26926/26926 [==============================] - 41s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 10/10\n",
      "26926/26926 [==============================] - 45s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "2\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 11/15\n",
      "26926/26926 [==============================] - 44s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 12/15\n",
      "26926/26926 [==============================] - 43s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 13/15\n",
      "26926/26926 [==============================] - 42s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 14/15\n",
      "26926/26926 [==============================] - 44s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 15/15\n",
      "26926/26926 [==============================] - 45s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "3\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 16/20\n",
      "26926/26926 [==============================] - 43s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 17/20\n",
      "26926/26926 [==============================] - 43s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 18/20\n",
      "26926/26926 [==============================] - 45s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 19/20\n",
      "26926/26926 [==============================] - 48s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 20/20\n",
      "26926/26926 [==============================] - 44s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "4\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 21/25\n",
      "26926/26926 [==============================] - 48s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 22/25\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 23/25\n",
      "26926/26926 [==============================] - 45s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 24/25\n",
      "26926/26926 [==============================] - 47s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 25/25\n",
      "26926/26926 [==============================] - 48s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "5\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 26/30\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 27/30\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 28/30\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 29/30\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 30/30\n",
      "26926/26926 [==============================] - 48s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "6\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 31/35\n",
      "26926/26926 [==============================] - 44s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 32/35\n",
      "26926/26926 [==============================] - 45s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 33/35\n",
      "26926/26926 [==============================] - 47s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 34/35\n",
      "26926/26926 [==============================] - 47s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 35/35\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "7\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 36/40\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 37/40\n",
      "26926/26926 [==============================] - 53s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 38/40\n",
      "26926/26926 [==============================] - 43s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 39/40\n",
      "26926/26926 [==============================] - 50s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 40/40\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "8\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 41/45\n",
      "26926/26926 [==============================] - 47s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 42/45\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 43/45\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 44/45\n",
      "26926/26926 [==============================] - 51s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 45/45\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "9\n",
      "Train on 26926 samples, validate on 11540 samples\n",
      "Epoch 46/50\n",
      "26926/26926 [==============================] - 53s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 47/50\n",
      "26926/26926 [==============================] - 46s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 48/50\n",
      "26926/26926 [==============================] - 42s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 49/50\n",
      "26926/26926 [==============================] - 41s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n",
      "Epoch 50/50\n",
      "26926/26926 [==============================] - 40s - loss: 2.9398 - categorical_accuracy: 0.8176 - val_loss: 2.9098 - val_categorical_accuracy: 0.8195\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i)\n",
    "    history = training_round(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel.save('../models/gameaction_ae64.hp5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel.load_weights('../models/gameaction_ae64.hp5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_arr = testmodel.predict(test_x[0].reshape(1, 16)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 301)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_arr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in y_arr[0]:\n",
    "    \n",
    "    print(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_arr.argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  1,  8],\n",
       "       [ 0,  0,  0, ...,  0,  1,  8],\n",
       "       [ 0,  0,  0, ...,  0,  1,  6],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  1,  6],\n",
       "       [ 0,  0,  0, ...,  0,  1,  4],\n",
       "       [ 0,  0,  0, ..., 13,  3, 71]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
