{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_params import *\n",
    "import text_encoder as te\n",
    "import text_decoder as td\n",
    "from data_set import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bowizer\n",
    "import tfidf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_policy_random.pkl', 'rb') as f:\n",
    "    gd = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n                    ________  ________  __    __  ________        \\n                   |        \\\\|        \\\\|  \\\\  |  \\\\|        \\\\       \\n                    \\\\$$$$$$$$| $$$$$$$$| $$  | $$ \\\\$$$$$$$$       \\n                      | $$   | $$__     \\\\$$\\\\/  $$   | $$          \\n                      | $$   | $$  \\\\     >$$  $$    | $$          \\n                      | $$   | $$$$$    /  $$$$\\\\    | $$          \\n                      | $$   | $$_____ |  $$ \\\\$$\\\\   | $$          \\n                      | $$   | $$     \\\\| $$  | $$   | $$          \\n                       \\\\$$    \\\\$$$$$$$$ \\\\$$   \\\\$$    \\\\$$          \\n              __       __   ______   _______   __        _______  \\n             |  \\\\  _  |  \\\\ /      \\\\ |       \\\\ |  \\\\      |       \\\\ \\n             | $$ / \\\\ | $$|  $$$$$$\\\\| $$$$$$$\\\\| $$      | $$$$$$$\\\\\\n             | $$/  $\\\\| $$| $$  | $$| $$__| $$| $$      | $$  | $$\\n             | $$  $$$\\\\ $$| $$  | $$| $$    $$| $$      | $$  | $$\\n             | $$ $$\\\\$$\\\\$$| $$  | $$| $$$$$$$\\\\| $$      | $$  | $$\\n             | $$$$  \\\\$$$$| $$__/ $$| $$  | $$| $$_____ | $$__/ $$\\n             | $$$    \\\\$$$ \\\\$$    $$| $$  | $$| $$     \\\\| $$    $$\\n              \\\\$$      \\\\$$  \\\\$$$$$$  \\\\$$   \\\\$$ \\\\$$$$$$$$ \\\\$$$$$$$ \\n\\nIt\\'s time to explore the amazing world of TextWorld! Make it so the safe inside the cellar is locked.\\n\\n-= Cellar =-\\nYou find yourself in a cellar. A typical one. You try to gain information on your surroundings by using a technique you call \"looking.\"\\n\\nA closed safe, which looks typical, is in the corner.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob ='\\n\\n\\n                    ________  ________  __    __  ________        \\n                   |        \\\\|        \\\\|  \\\\  |  \\\\|        \\\\       \\n                    \\\\$$$$$$$$| $$$$$$$$| $$  | $$ \\\\$$$$$$$$       \\n                      | $$   | $$__     \\\\$$\\\\/  $$   | $$          \\n                      | $$   | $$  \\\\     >$$  $$    | $$          \\n                      | $$   | $$$$$    /  $$$$\\\\    | $$          \\n                      | $$   | $$_____ |  $$ \\\\$$\\\\   | $$          \\n                      | $$   | $$     \\\\| $$  | $$   | $$          \\n                       \\\\$$    \\\\$$$$$$$$ \\\\$$   \\\\$$    \\\\$$          \\n              __       __   ______   _______   __        _______  \\n             |  \\\\  _  |  \\\\ /      \\\\ |       \\\\ |  \\\\      |       \\\\ \\n             | $$ / \\\\ | $$|  $$$$$$\\\\| $$$$$$$\\\\| $$      | $$$$$$$\\\\\\n             | $$/  $\\\\| $$| $$  | $$| $$__| $$| $$      | $$  | $$\\n             | $$  $$$\\\\ $$| $$  | $$| $$    $$| $$      | $$  | $$\\n             | $$ $$\\\\$$\\\\$$| $$  | $$| $$$$$$$\\\\| $$      | $$  | $$\\n             | $$$$  \\\\$$$$| $$__/ $$| $$  | $$| $$_____ | $$__/ $$\\n             | $$$    \\\\$$$ \\\\$$    $$| $$  | $$| $$     \\\\| $$    $$\\n              \\\\$$      \\\\$$  \\\\$$$$$$  \\\\$$   \\\\$$ \\\\$$$$$$$$ \\\\$$$$$$$ \\n\\n'\n",
    "bloblen = len(blob)\n",
    "\n",
    "gd[0] = [x.replace(blob, \"\") for x in gd[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n-= Workshop =-\\nYou've entered a workshop. You begin to take stock of what's in the room.\\n\\n\\n\\nThere is an exit to the north. Don't worry, it is unguarded. There is an exit to the west. Don't worry, it is unblocked.\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_data = len(gd[0])\n",
    "\n",
    "vocab_size = 3000\n",
    "training_split = 0.7\n",
    "\n",
    "traininglen = int(len_data * training_split)\n",
    "\n",
    "# traininglen = 2000\n",
    "training_set = gd[0][:traininglen]\n",
    "test_set = gd[0][traininglen:]\n",
    "test_set[0]\n",
    "\n",
    "\n",
    "#tm = bowizer.TokenMaker(corpus=gd[0], vocab_size= vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('norvig_tm_3000.pkl', 'rb') as f:\n",
    "    tm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_h = EmbeddingHyper(tm.vocab_size + 1, 256)\n",
    "conv_h = ConvHyper(256, 6, 4)\n",
    "rnn_h = RnnHyper(512, is_lstm=False, is_bidirectional=True, return_sequences=False)\n",
    "encoder_h = te.Hyper(embed_h, [conv_h, rnn_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dernn_h = RnnHyper(512, is_lstm=False, is_bidirectional=False, return_sequences=True, unroll=True)\n",
    "dec_h = DeconvHyper(256, 6, 4)\n",
    "decoder_h = td.Hyper(tm.vocab_size + 1, [dernn_h, dec_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder_h.make_layer()\n",
    "decoder = decoder_h.make_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(training_set).shape\n",
    "\n",
    "ml = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(max_len):\n",
    "    x = Input(shape=(max_len,), name='text_input')\n",
    "    h = encoder(x)\n",
    "    h = decoder(h, max_len)\n",
    "    model = Model(x, h)\n",
    "    model.compile(optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel = make_model(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel.load_weights('../models/wl_model64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_round(history=None):\n",
    "    if history is None:\n",
    "        initial_epoch = 0\n",
    "    else:\n",
    "        initial_epoch = len(history['loss'])\n",
    "    train = bowizer.SlicedWordData(lines=r.choice(training_set, size=2000, replace=False), maxlen=ml,tokenmaker=tm)\n",
    "    test = bowizer.SlicedWordData(lines=r.choice(test_set, size=400, replace=False), maxlen=ml,tokenmaker=tm)\n",
    "    newhistory = testmodel.fit(x=train.x, y=train.y,\n",
    "                            epochs=initial_epoch+5, batch_size=32,\n",
    "                            validation_data=(test.x, test.y),\n",
    "                            initial_epoch=initial_epoch)\n",
    "    if history is None:\n",
    "        history = newhistory.history\n",
    "    else:\n",
    "        history = {key:history[key] + newhistory.history[key] for key in history.keys()}\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 44s - loss: 1.4378 - categorical_accuracy: 0.6869 - val_loss: 1.5296 - val_categorical_accuracy: 0.6629\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 43s - loss: 1.3764 - categorical_accuracy: 0.6920 - val_loss: 1.4511 - val_categorical_accuracy: 0.6741\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 43s - loss: 1.3099 - categorical_accuracy: 0.7008 - val_loss: 1.3845 - val_categorical_accuracy: 0.6846\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 43s - loss: 1.2866 - categorical_accuracy: 0.7049 - val_loss: 1.3736 - val_categorical_accuracy: 0.6872\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 43s - loss: 1.2631 - categorical_accuracy: 0.7068 - val_loss: 1.3212 - val_categorical_accuracy: 0.6992\n",
      "1\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 44s - loss: 1.2552 - categorical_accuracy: 0.7121 - val_loss: 1.2427 - val_categorical_accuracy: 0.7116\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 43s - loss: 1.1865 - categorical_accuracy: 0.7252 - val_loss: 1.1899 - val_categorical_accuracy: 0.7301\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 44s - loss: 1.1700 - categorical_accuracy: 0.7258 - val_loss: 1.1711 - val_categorical_accuracy: 0.7342\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 43s - loss: 1.1267 - categorical_accuracy: 0.7347 - val_loss: 1.2024 - val_categorical_accuracy: 0.7248\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 43s - loss: 1.1273 - categorical_accuracy: 0.7301 - val_loss: 1.1197 - val_categorical_accuracy: 0.7417\n",
      "2\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 11/15\n",
      "2000/2000 [==============================] - 44s - loss: 1.0696 - categorical_accuracy: 0.7482 - val_loss: 1.0594 - val_categorical_accuracy: 0.7522\n",
      "Epoch 12/15\n",
      "2000/2000 [==============================] - 44s - loss: 1.0384 - categorical_accuracy: 0.7522 - val_loss: 1.0612 - val_categorical_accuracy: 0.7511\n",
      "Epoch 13/15\n",
      "2000/2000 [==============================] - 44s - loss: 1.0163 - categorical_accuracy: 0.7555 - val_loss: 1.0119 - val_categorical_accuracy: 0.7635\n",
      "Epoch 14/15\n",
      "2000/2000 [==============================] - 43s - loss: 0.9624 - categorical_accuracy: 0.7672 - val_loss: 1.1318 - val_categorical_accuracy: 0.7278\n",
      "Epoch 15/15\n",
      "2000/2000 [==============================] - 43s - loss: 0.9877 - categorical_accuracy: 0.7589 - val_loss: 1.0208 - val_categorical_accuracy: 0.7582\n",
      "3\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 16/20\n",
      "2000/2000 [==============================] - 44s - loss: 1.0355 - categorical_accuracy: 0.7539 - val_loss: 1.1087 - val_categorical_accuracy: 0.7367\n",
      "Epoch 17/20\n",
      "2000/2000 [==============================] - 44s - loss: 1.0750 - categorical_accuracy: 0.7380 - val_loss: 1.1136 - val_categorical_accuracy: 0.7317\n",
      "Epoch 18/20\n",
      "2000/2000 [==============================] - 43s - loss: 0.9826 - categorical_accuracy: 0.7611 - val_loss: 1.1368 - val_categorical_accuracy: 0.7267\n",
      "Epoch 19/20\n",
      "2000/2000 [==============================] - 44s - loss: 0.9549 - categorical_accuracy: 0.7666 - val_loss: 1.0592 - val_categorical_accuracy: 0.7456\n",
      "Epoch 20/20\n",
      "2000/2000 [==============================] - 44s - loss: 0.9300 - categorical_accuracy: 0.7704 - val_loss: 1.0587 - val_categorical_accuracy: 0.7436\n",
      "4\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 21/25\n",
      "2000/2000 [==============================] - 45s - loss: 0.9956 - categorical_accuracy: 0.7580 - val_loss: 1.0412 - val_categorical_accuracy: 0.7536\n",
      "Epoch 22/25\n",
      "2000/2000 [==============================] - 44s - loss: 0.9990 - categorical_accuracy: 0.7524 - val_loss: 1.0817 - val_categorical_accuracy: 0.7463\n",
      "Epoch 23/25\n",
      "2000/2000 [==============================] - 43s - loss: 0.9322 - categorical_accuracy: 0.7690 - val_loss: 1.0094 - val_categorical_accuracy: 0.7576\n",
      "Epoch 24/25\n",
      "2000/2000 [==============================] - 44s - loss: 0.8910 - categorical_accuracy: 0.7753 - val_loss: 1.0018 - val_categorical_accuracy: 0.7596\n",
      "Epoch 25/25\n",
      "2000/2000 [==============================] - 44s - loss: 0.8699 - categorical_accuracy: 0.7782 - val_loss: 0.9980 - val_categorical_accuracy: 0.7594\n",
      "5\n",
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 26/30\n",
      "1408/2000 [====================>.........] - ETA: 12s - loss: 0.9211 - categorical_accuracy: 0.7707"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(i)\n",
    "    history = training_round(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 1/5\n",
      "2000/2000 [==============================] - 46s - loss: 2.4730 - categorical_accuracy: 0.6573 - val_loss: 1.8280 - val_categorical_accuracy: 0.6826\n",
      "Epoch 2/5\n",
      "2000/2000 [==============================] - 44s - loss: 1.7381 - categorical_accuracy: 0.6790 - val_loss: 1.6597 - val_categorical_accuracy: 0.6852\n",
      "Epoch 3/5\n",
      "2000/2000 [==============================] - 43s - loss: 1.5909 - categorical_accuracy: 0.6829 - val_loss: 1.5535 - val_categorical_accuracy: 0.6862\n",
      "Epoch 4/5\n",
      "2000/2000 [==============================] - 44s - loss: 1.5106 - categorical_accuracy: 0.6854 - val_loss: 1.4956 - val_categorical_accuracy: 0.6894\n",
      "Epoch 5/5\n",
      "2000/2000 [==============================] - 43s - loss: 1.4549 - categorical_accuracy: 0.6878 - val_loss: 1.4695 - val_categorical_accuracy: 0.6864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a8169af98>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel.fit(x=train.x, y=train.y,\n",
    "                            epochs=5, batch_size=32,\n",
    "                            validation_data=(test.x, test.y)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 400 samples\n",
      "Epoch 1/150\n",
      "2000/2000 [==============================] - 22s - loss: 0.4866 - categorical_accuracy: 0.8453 - val_loss: 2.0084 - val_categorical_accuracy: 0.6918\n",
      "Epoch 2/150\n",
      "2000/2000 [==============================] - 22s - loss: 0.3584 - categorical_accuracy: 0.8887 - val_loss: 2.0546 - val_categorical_accuracy: 0.6971\n",
      "Epoch 3/150\n",
      "2000/2000 [==============================] - 23s - loss: 0.3081 - categorical_accuracy: 0.9097 - val_loss: 2.1250 - val_categorical_accuracy: 0.6963\n",
      "Epoch 4/150\n",
      "2000/2000 [==============================] - 23s - loss: 0.2800 - categorical_accuracy: 0.9214 - val_loss: 2.1742 - val_categorical_accuracy: 0.6964\n",
      "Epoch 5/150\n",
      "2000/2000 [==============================] - 23s - loss: 0.2598 - categorical_accuracy: 0.9292 - val_loss: 2.2207 - val_categorical_accuracy: 0.6980\n",
      "Epoch 6/150\n",
      "2000/2000 [==============================] - 24s - loss: 0.2411 - categorical_accuracy: 0.9364 - val_loss: 2.2806 - val_categorical_accuracy: 0.6957\n",
      "Epoch 7/150\n",
      "2000/2000 [==============================] - 24s - loss: 0.2279 - categorical_accuracy: 0.9417 - val_loss: 2.3254 - val_categorical_accuracy: 0.6965\n",
      "Epoch 8/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.2192 - categorical_accuracy: 0.9438 - val_loss: 2.3775 - val_categorical_accuracy: 0.6938\n",
      "Epoch 9/150\n",
      "2000/2000 [==============================] - 24s - loss: 0.2092 - categorical_accuracy: 0.9448 - val_loss: 2.4147 - val_categorical_accuracy: 0.6930\n",
      "Epoch 10/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.2055 - categorical_accuracy: 0.9446 - val_loss: 2.4673 - val_categorical_accuracy: 0.6948\n",
      "Epoch 11/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.1959 - categorical_accuracy: 0.9458 - val_loss: 2.5232 - val_categorical_accuracy: 0.6960\n",
      "Epoch 12/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.1874 - categorical_accuracy: 0.9485 - val_loss: 2.5729 - val_categorical_accuracy: 0.6876\n",
      "Epoch 13/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.2028 - categorical_accuracy: 0.9390 - val_loss: 2.5685 - val_categorical_accuracy: 0.6929\n",
      "Epoch 14/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.1753 - categorical_accuracy: 0.9517 - val_loss: 2.6069 - val_categorical_accuracy: 0.6910\n",
      "Epoch 15/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.1634 - categorical_accuracy: 0.9563 - val_loss: 2.7055 - val_categorical_accuracy: 0.6875\n",
      "Epoch 16/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.2567 - categorical_accuracy: 0.9325 - val_loss: 2.9752 - val_categorical_accuracy: 0.6548\n",
      "Epoch 17/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.8616 - categorical_accuracy: 0.7707 - val_loss: 2.3416 - val_categorical_accuracy: 0.6796\n",
      "Epoch 18/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.4290 - categorical_accuracy: 0.8610 - val_loss: 2.3919 - val_categorical_accuracy: 0.6906\n",
      "Epoch 19/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.2514 - categorical_accuracy: 0.9239 - val_loss: 2.4512 - val_categorical_accuracy: 0.6953\n",
      "Epoch 20/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.1639 - categorical_accuracy: 0.9582 - val_loss: 2.5460 - val_categorical_accuracy: 0.6933\n",
      "Epoch 21/150\n",
      "2000/2000 [==============================] - 24s - loss: 0.1216 - categorical_accuracy: 0.9759 - val_loss: 2.6066 - val_categorical_accuracy: 0.6919\n",
      "Epoch 22/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0959 - categorical_accuracy: 0.9850 - val_loss: 2.6539 - val_categorical_accuracy: 0.6939\n",
      "Epoch 23/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0808 - categorical_accuracy: 0.9902 - val_loss: 2.7117 - val_categorical_accuracy: 0.6927\n",
      "Epoch 24/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0699 - categorical_accuracy: 0.9928 - val_loss: 2.7605 - val_categorical_accuracy: 0.6928\n",
      "Epoch 25/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0627 - categorical_accuracy: 0.9943 - val_loss: 2.8080 - val_categorical_accuracy: 0.6923\n",
      "Epoch 26/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0574 - categorical_accuracy: 0.9949 - val_loss: 2.8510 - val_categorical_accuracy: 0.6921\n",
      "Epoch 27/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0530 - categorical_accuracy: 0.9953 - val_loss: 2.8921 - val_categorical_accuracy: 0.6922\n",
      "Epoch 28/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0499 - categorical_accuracy: 0.9953 - val_loss: 2.9154 - val_categorical_accuracy: 0.6927\n",
      "Epoch 29/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0488 - categorical_accuracy: 0.9947 - val_loss: 2.9608 - val_categorical_accuracy: 0.6911\n",
      "Epoch 30/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0551 - categorical_accuracy: 0.9925 - val_loss: 2.9736 - val_categorical_accuracy: 0.6926\n",
      "Epoch 31/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0998 - categorical_accuracy: 0.9770 - val_loss: 2.9716 - val_categorical_accuracy: 0.6890\n",
      "Epoch 32/150\n",
      "2000/2000 [==============================] - 24s - loss: 0.1991 - categorical_accuracy: 0.9500 - val_loss: 3.2029 - val_categorical_accuracy: 0.6619\n",
      "Epoch 33/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.9972 - categorical_accuracy: 0.7584 - val_loss: 2.4106 - val_categorical_accuracy: 0.6766\n",
      "Epoch 34/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.6687 - categorical_accuracy: 0.8057 - val_loss: 2.3714 - val_categorical_accuracy: 0.6857\n",
      "Epoch 35/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.3909 - categorical_accuracy: 0.8774 - val_loss: 2.5327 - val_categorical_accuracy: 0.6841\n",
      "Epoch 36/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.3193 - categorical_accuracy: 0.9047 - val_loss: 2.5116 - val_categorical_accuracy: 0.6910\n",
      "Epoch 37/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.1958 - categorical_accuracy: 0.9452 - val_loss: 2.6033 - val_categorical_accuracy: 0.6905\n",
      "Epoch 38/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.1136 - categorical_accuracy: 0.9766 - val_loss: 2.6518 - val_categorical_accuracy: 0.6953\n",
      "Epoch 39/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0697 - categorical_accuracy: 0.9916 - val_loss: 2.7239 - val_categorical_accuracy: 0.6950\n",
      "Epoch 40/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0523 - categorical_accuracy: 0.9960 - val_loss: 2.7723 - val_categorical_accuracy: 0.6962\n",
      "Epoch 41/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0414 - categorical_accuracy: 0.9976 - val_loss: 2.8188 - val_categorical_accuracy: 0.6956\n",
      "Epoch 42/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0354 - categorical_accuracy: 0.9983 - val_loss: 2.8607 - val_categorical_accuracy: 0.6963\n",
      "Epoch 43/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0313 - categorical_accuracy: 0.9986 - val_loss: 2.8955 - val_categorical_accuracy: 0.6963\n",
      "Epoch 44/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0281 - categorical_accuracy: 0.9986 - val_loss: 2.9244 - val_categorical_accuracy: 0.6968\n",
      "Epoch 45/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0259 - categorical_accuracy: 0.9987 - val_loss: 2.9591 - val_categorical_accuracy: 0.6958\n",
      "Epoch 46/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0229 - categorical_accuracy: 0.9990 - val_loss: 2.9869 - val_categorical_accuracy: 0.6960\n",
      "Epoch 47/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0209 - categorical_accuracy: 0.9991 - val_loss: 3.0145 - val_categorical_accuracy: 0.6961\n",
      "Epoch 48/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0192 - categorical_accuracy: 0.9991 - val_loss: 3.0361 - val_categorical_accuracy: 0.6960\n",
      "Epoch 49/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0178 - categorical_accuracy: 0.9991 - val_loss: 3.0613 - val_categorical_accuracy: 0.6958\n",
      "Epoch 50/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0165 - categorical_accuracy: 0.9991 - val_loss: 3.0833 - val_categorical_accuracy: 0.6962\n",
      "Epoch 51/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0153 - categorical_accuracy: 0.9992 - val_loss: 3.1058 - val_categorical_accuracy: 0.6964\n",
      "Epoch 52/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0144 - categorical_accuracy: 0.9992 - val_loss: 3.1245 - val_categorical_accuracy: 0.6952\n",
      "Epoch 53/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0134 - categorical_accuracy: 0.9991 - val_loss: 3.1447 - val_categorical_accuracy: 0.6955\n",
      "Epoch 54/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0129 - categorical_accuracy: 0.9991 - val_loss: 3.1614 - val_categorical_accuracy: 0.6952\n",
      "Epoch 55/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0120 - categorical_accuracy: 0.9991 - val_loss: 3.1803 - val_categorical_accuracy: 0.6951\n",
      "Epoch 56/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0113 - categorical_accuracy: 0.9992 - val_loss: 3.1950 - val_categorical_accuracy: 0.6955\n",
      "Epoch 57/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0110 - categorical_accuracy: 0.9991 - val_loss: 3.2132 - val_categorical_accuracy: 0.6942\n",
      "Epoch 58/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0107 - categorical_accuracy: 0.9991 - val_loss: 3.2311 - val_categorical_accuracy: 0.6951\n",
      "Epoch 59/150\n",
      "2000/2000 [==============================] - 24s - loss: 0.0118 - categorical_accuracy: 0.9988 - val_loss: 3.2463 - val_categorical_accuracy: 0.6948\n",
      "Epoch 60/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0372 - categorical_accuracy: 0.9928 - val_loss: 3.2287 - val_categorical_accuracy: 0.6874\n",
      "Epoch 61/150\n",
      "2000/2000 [==============================] - 26s - loss: 1.0036 - categorical_accuracy: 0.8001 - val_loss: 2.3622 - val_categorical_accuracy: 0.6490\n",
      "Epoch 62/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.7900 - categorical_accuracy: 0.7818 - val_loss: 2.5333 - val_categorical_accuracy: 0.6772\n",
      "Epoch 63/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.4060 - categorical_accuracy: 0.8793 - val_loss: 2.7237 - val_categorical_accuracy: 0.6575\n",
      "Epoch 64/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.3749 - categorical_accuracy: 0.8901 - val_loss: 2.7004 - val_categorical_accuracy: 0.6753\n",
      "Epoch 65/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.2177 - categorical_accuracy: 0.9429 - val_loss: 2.7568 - val_categorical_accuracy: 0.6874\n",
      "Epoch 66/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0927 - categorical_accuracy: 0.9827 - val_loss: 2.8533 - val_categorical_accuracy: 0.6909\n",
      "Epoch 67/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0560 - categorical_accuracy: 0.9934 - val_loss: 2.8959 - val_categorical_accuracy: 0.6911\n",
      "Epoch 68/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0436 - categorical_accuracy: 0.9955 - val_loss: 2.9271 - val_categorical_accuracy: 0.6912\n",
      "Epoch 69/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0422 - categorical_accuracy: 0.9955 - val_loss: 2.9653 - val_categorical_accuracy: 0.6926\n",
      "Epoch 70/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0333 - categorical_accuracy: 0.9960 - val_loss: 3.0081 - val_categorical_accuracy: 0.6920\n",
      "Epoch 71/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0265 - categorical_accuracy: 0.9970 - val_loss: 3.0418 - val_categorical_accuracy: 0.6930\n",
      "Epoch 72/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0234 - categorical_accuracy: 0.9972 - val_loss: 3.0680 - val_categorical_accuracy: 0.6928\n",
      "Epoch 73/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0228 - categorical_accuracy: 0.9972 - val_loss: 3.0891 - val_categorical_accuracy: 0.6920\n",
      "Epoch 74/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0208 - categorical_accuracy: 0.9973 - val_loss: 3.1169 - val_categorical_accuracy: 0.6917\n",
      "Epoch 75/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0190 - categorical_accuracy: 0.9974 - val_loss: 3.1387 - val_categorical_accuracy: 0.6920\n",
      "Epoch 76/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0185 - categorical_accuracy: 0.9974 - val_loss: 3.1551 - val_categorical_accuracy: 0.6918\n",
      "Epoch 77/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0176 - categorical_accuracy: 0.9974 - val_loss: 3.1738 - val_categorical_accuracy: 0.6922\n",
      "Epoch 78/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0166 - categorical_accuracy: 0.9975 - val_loss: 3.1887 - val_categorical_accuracy: 0.6926\n",
      "Epoch 79/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0163 - categorical_accuracy: 0.9975 - val_loss: 3.2052 - val_categorical_accuracy: 0.6918\n",
      "Epoch 80/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0158 - categorical_accuracy: 0.9975 - val_loss: 3.2240 - val_categorical_accuracy: 0.6922\n",
      "Epoch 81/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0148 - categorical_accuracy: 0.9975 - val_loss: 3.2393 - val_categorical_accuracy: 0.6918\n",
      "Epoch 82/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0150 - categorical_accuracy: 0.9974 - val_loss: 3.2510 - val_categorical_accuracy: 0.6914\n",
      "Epoch 83/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0156 - categorical_accuracy: 0.9973 - val_loss: 3.2683 - val_categorical_accuracy: 0.6911\n",
      "Epoch 84/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0156 - categorical_accuracy: 0.9973 - val_loss: 3.2736 - val_categorical_accuracy: 0.6917\n",
      "Epoch 85/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0177 - categorical_accuracy: 0.9970 - val_loss: 3.2859 - val_categorical_accuracy: 0.6890\n",
      "Epoch 86/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.1168 - categorical_accuracy: 0.9774 - val_loss: 3.3048 - val_categorical_accuracy: 0.6757\n",
      "Epoch 87/150\n",
      "2000/2000 [==============================] - 25s - loss: 1.2046 - categorical_accuracy: 0.7262 - val_loss: 2.2560 - val_categorical_accuracy: 0.6396\n",
      "Epoch 88/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.7663 - categorical_accuracy: 0.7865 - val_loss: 2.5318 - val_categorical_accuracy: 0.6771\n",
      "Epoch 89/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.4129 - categorical_accuracy: 0.8766 - val_loss: 2.6161 - val_categorical_accuracy: 0.6790\n",
      "Epoch 90/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.2349 - categorical_accuracy: 0.9319 - val_loss: 2.7408 - val_categorical_accuracy: 0.6827\n",
      "Epoch 91/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.2062 - categorical_accuracy: 0.9432 - val_loss: 2.8021 - val_categorical_accuracy: 0.6804\n",
      "Epoch 92/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.1361 - categorical_accuracy: 0.9674 - val_loss: 2.8553 - val_categorical_accuracy: 0.6844\n",
      "Epoch 93/150\n",
      "2000/2000 [==============================] - 28s - loss: 0.1940 - categorical_accuracy: 0.9499 - val_loss: 2.8740 - val_categorical_accuracy: 0.6670\n",
      "Epoch 94/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.2200 - categorical_accuracy: 0.9402 - val_loss: 2.8972 - val_categorical_accuracy: 0.6826\n",
      "Epoch 95/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.1520 - categorical_accuracy: 0.9645 - val_loss: 2.9270 - val_categorical_accuracy: 0.6796\n",
      "Epoch 96/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0918 - categorical_accuracy: 0.9821 - val_loss: 2.9660 - val_categorical_accuracy: 0.6830\n",
      "Epoch 97/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0473 - categorical_accuracy: 0.9922 - val_loss: 3.0270 - val_categorical_accuracy: 0.6846\n",
      "Epoch 98/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0411 - categorical_accuracy: 0.9937 - val_loss: 3.0726 - val_categorical_accuracy: 0.6844\n",
      "Epoch 99/150\n",
      "2000/2000 [==============================] - 28s - loss: 0.0421 - categorical_accuracy: 0.9931 - val_loss: 3.0957 - val_categorical_accuracy: 0.6842\n",
      "Epoch 100/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0361 - categorical_accuracy: 0.9946 - val_loss: 3.1434 - val_categorical_accuracy: 0.6848\n",
      "Epoch 101/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 26s - loss: 0.0252 - categorical_accuracy: 0.9963 - val_loss: 3.1772 - val_categorical_accuracy: 0.6865\n",
      "Epoch 102/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0218 - categorical_accuracy: 0.9966 - val_loss: 3.2042 - val_categorical_accuracy: 0.6858\n",
      "Epoch 103/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0205 - categorical_accuracy: 0.9967 - val_loss: 3.2280 - val_categorical_accuracy: 0.6862\n",
      "Epoch 104/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0200 - categorical_accuracy: 0.9967 - val_loss: 3.2487 - val_categorical_accuracy: 0.6859\n",
      "Epoch 105/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0197 - categorical_accuracy: 0.9966 - val_loss: 3.2704 - val_categorical_accuracy: 0.6857\n",
      "Epoch 106/150\n",
      "2000/2000 [==============================] - 24s - loss: 0.0187 - categorical_accuracy: 0.9967 - val_loss: 3.2880 - val_categorical_accuracy: 0.6851\n",
      "Epoch 107/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0187 - categorical_accuracy: 0.9967 - val_loss: 3.3072 - val_categorical_accuracy: 0.6853\n",
      "Epoch 108/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0185 - categorical_accuracy: 0.9967 - val_loss: 3.3191 - val_categorical_accuracy: 0.6857\n",
      "Epoch 109/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0173 - categorical_accuracy: 0.9967 - val_loss: 3.3342 - val_categorical_accuracy: 0.6850\n",
      "Epoch 110/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0167 - categorical_accuracy: 0.9967 - val_loss: 3.3487 - val_categorical_accuracy: 0.6848\n",
      "Epoch 111/150\n",
      "2000/2000 [==============================] - 28s - loss: 0.0171 - categorical_accuracy: 0.9967 - val_loss: 3.3632 - val_categorical_accuracy: 0.6850\n",
      "Epoch 112/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0167 - categorical_accuracy: 0.9967 - val_loss: 3.3749 - val_categorical_accuracy: 0.6851\n",
      "Epoch 113/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0162 - categorical_accuracy: 0.9967 - val_loss: 3.3887 - val_categorical_accuracy: 0.6850\n",
      "Epoch 114/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0162 - categorical_accuracy: 0.9967 - val_loss: 3.4000 - val_categorical_accuracy: 0.6856\n",
      "Epoch 115/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0163 - categorical_accuracy: 0.9967 - val_loss: 3.4124 - val_categorical_accuracy: 0.6853\n",
      "Epoch 116/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0155 - categorical_accuracy: 0.9968 - val_loss: 3.4215 - val_categorical_accuracy: 0.6856\n",
      "Epoch 117/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0153 - categorical_accuracy: 0.9967 - val_loss: 3.4339 - val_categorical_accuracy: 0.6839\n",
      "Epoch 118/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0170 - categorical_accuracy: 0.9967 - val_loss: 3.4409 - val_categorical_accuracy: 0.6848\n",
      "Epoch 119/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0160 - categorical_accuracy: 0.9967 - val_loss: 3.4503 - val_categorical_accuracy: 0.6844\n",
      "Epoch 120/150\n",
      "2000/2000 [==============================] - 28s - loss: 0.0158 - categorical_accuracy: 0.9966 - val_loss: 3.4613 - val_categorical_accuracy: 0.6846\n",
      "Epoch 121/150\n",
      "2000/2000 [==============================] - 28s - loss: 0.0149 - categorical_accuracy: 0.9967 - val_loss: 3.4724 - val_categorical_accuracy: 0.6847\n",
      "Epoch 122/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0147 - categorical_accuracy: 0.9967 - val_loss: 3.4814 - val_categorical_accuracy: 0.6853\n",
      "Epoch 123/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0146 - categorical_accuracy: 0.9967 - val_loss: 3.4929 - val_categorical_accuracy: 0.6849\n",
      "Epoch 124/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0144 - categorical_accuracy: 0.9967 - val_loss: 3.5027 - val_categorical_accuracy: 0.6845\n",
      "Epoch 125/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0144 - categorical_accuracy: 0.9968 - val_loss: 3.5106 - val_categorical_accuracy: 0.6847\n",
      "Epoch 126/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0143 - categorical_accuracy: 0.9967 - val_loss: 3.5200 - val_categorical_accuracy: 0.6851\n",
      "Epoch 127/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0143 - categorical_accuracy: 0.9968 - val_loss: 3.5291 - val_categorical_accuracy: 0.6845\n",
      "Epoch 128/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0141 - categorical_accuracy: 0.9967 - val_loss: 3.5369 - val_categorical_accuracy: 0.6847\n",
      "Epoch 129/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0141 - categorical_accuracy: 0.9967 - val_loss: 3.5450 - val_categorical_accuracy: 0.6843\n",
      "Epoch 130/150\n",
      "2000/2000 [==============================] - 28s - loss: 0.0137 - categorical_accuracy: 0.9968 - val_loss: 3.5554 - val_categorical_accuracy: 0.6846\n",
      "Epoch 131/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0137 - categorical_accuracy: 0.9968 - val_loss: 3.5601 - val_categorical_accuracy: 0.6842\n",
      "Epoch 132/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0137 - categorical_accuracy: 0.9968 - val_loss: 3.5681 - val_categorical_accuracy: 0.6846\n",
      "Epoch 133/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0136 - categorical_accuracy: 0.9967 - val_loss: 3.5760 - val_categorical_accuracy: 0.6846\n",
      "Epoch 134/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0149 - categorical_accuracy: 0.9967 - val_loss: 3.5847 - val_categorical_accuracy: 0.6848\n",
      "Epoch 135/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0141 - categorical_accuracy: 0.9967 - val_loss: 3.5870 - val_categorical_accuracy: 0.6846\n",
      "Epoch 136/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0138 - categorical_accuracy: 0.9968 - val_loss: 3.5983 - val_categorical_accuracy: 0.6840\n",
      "Epoch 137/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0136 - categorical_accuracy: 0.9967 - val_loss: 3.6054 - val_categorical_accuracy: 0.6840\n",
      "Epoch 138/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0136 - categorical_accuracy: 0.9968 - val_loss: 3.6119 - val_categorical_accuracy: 0.6843\n",
      "Epoch 139/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0134 - categorical_accuracy: 0.9968 - val_loss: 3.6181 - val_categorical_accuracy: 0.6842\n",
      "Epoch 140/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0131 - categorical_accuracy: 0.9968 - val_loss: 3.6262 - val_categorical_accuracy: 0.6842\n",
      "Epoch 141/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0130 - categorical_accuracy: 0.9968 - val_loss: 3.6327 - val_categorical_accuracy: 0.6843\n",
      "Epoch 142/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0144 - categorical_accuracy: 0.9967 - val_loss: 3.6365 - val_categorical_accuracy: 0.6851\n",
      "Epoch 143/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0138 - categorical_accuracy: 0.9966 - val_loss: 3.6441 - val_categorical_accuracy: 0.6852\n",
      "Epoch 144/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0143 - categorical_accuracy: 0.9966 - val_loss: 3.6516 - val_categorical_accuracy: 0.6852\n",
      "Epoch 145/150\n",
      "2000/2000 [==============================] - 25s - loss: 0.0134 - categorical_accuracy: 0.9967 - val_loss: 3.6608 - val_categorical_accuracy: 0.6840\n",
      "Epoch 146/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0134 - categorical_accuracy: 0.9967 - val_loss: 3.6646 - val_categorical_accuracy: 0.6839\n",
      "Epoch 147/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0136 - categorical_accuracy: 0.9967 - val_loss: 3.6724 - val_categorical_accuracy: 0.6842\n",
      "Epoch 148/150\n",
      "2000/2000 [==============================] - 27s - loss: 0.0136 - categorical_accuracy: 0.9967 - val_loss: 3.6862 - val_categorical_accuracy: 0.6849\n",
      "Epoch 149/150\n",
      "2000/2000 [==============================] - 26s - loss: 0.0276 - categorical_accuracy: 0.9934 - val_loss: 3.6724 - val_categorical_accuracy: 0.6770\n",
      "Epoch 150/150\n",
      "2000/2000 [==============================] - 25s - loss: 1.4661 - categorical_accuracy: 0.6998 - val_loss: 2.1149 - val_categorical_accuracy: 0.6613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed053f7f28>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel.fit(x=train.x, y=train.y,\n",
    "                            epochs=150, batch_size=32,\n",
    "                            validation_data=(test.x, test.y)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
