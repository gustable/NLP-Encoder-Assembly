{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING TEXT FOR NATURAL LANGUAGE GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GETTING AND LOADING A CORPUS FROM GUTENBERG\n",
    "\n",
    "https://nlpforhackers.io/corpora/\n",
    "http://lucumr.pocoo.org/2015/11/18/pythons-hidden-re-gems/\n",
    "\n",
    "The goal of this step is to develop an initial list of each character and their spoken lines, or a cleanish list of the lines within the text. (Dictionaries are Hash value arbitrary, so may not be ordered the same. Lists are used instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and Read a file into \"f\" using a list and stripping out all Project Gutenberg headers and footers\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "data_folder = Path(\"data/murray_kempton/\")\n",
    "file_to_open = data_folder / \"On_Frank_Sinatra.txt\"\n",
    "f = open(file_to_open, 'r')\n",
    "first_document = list(f)\n",
    "#print(first_document)\n",
    "\n",
    "# Determine whether a Project Gutenberg Text\n",
    "first_header_index = 0\n",
    "second_header_index = 0\n",
    "footer_index = 0\n",
    "if any(\"GUTENBERG\" in s for s in first_document):\n",
    "    for first_header_index in range( len(first_document) ):\n",
    "        if ( ( first_document[first_header_index].find('*END*THE SMALL PRINT!') ) != -1 ) :\n",
    "            break\n",
    "        else:\n",
    "            for first_header_index in range( len(first_document) ):\n",
    "                if ( ( first_document[first_header_index].find('START OF THIS PROJECT GUTENBERG') ) != -1 ) :\n",
    "                    break        \n",
    "        \n",
    "    second_document = list(first_document[first_header_index + 1 :])\n",
    "\n",
    "    for second_header_index in range( len(second_document) ):\n",
    "        if ( ( second_document[second_header_index].find('www.gutenberg.org') ) != -1 ) :\n",
    "            break            \n",
    "    for footer_index in range( len(first_document) ):\n",
    "        if ( ( first_document[footer_index].find('End of Project') ) != -1 ) :\n",
    "            break\n",
    "        else:\n",
    "            for footer_index in range( len(first_document) ):\n",
    "                if ( ( first_document[footer_index].find('End of the Project') ) != -1 ) :\n",
    "                    break    \n",
    "        \n",
    "    print(first_header_index)            \n",
    "    print(second_header_index)\n",
    "    print(footer_index)     \n",
    "    \n",
    "    script = list()\n",
    "    if (second_header_index < (first_header_index + 100)):\n",
    "        script = list(first_document[first_header_index +1 + second_header_index +1 : footer_index-1])\n",
    "    else:\n",
    "        script = list(first_document[first_header_index +1 : footer_index-1])\n",
    "else:\n",
    "    script = first_document\n",
    "\n",
    "#print(script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDENTIFYING SPEAKERS AND PARTS FROM SCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Murray Kempton', 'ADVERTISING', 'Whitney Balliett']\n"
     ]
    }
   ],
   "source": [
    "#Compile a list of speakers\n",
    "r = \"([A-Z]+[a-z]+[ ]+[A-Za-z_]+\\\\n+)|([A-Z0-9][A-Z0-9]+\\\\n+)\"\n",
    "rg = re.compile(r, re.MULTILINE)\n",
    "speakers = []\n",
    "for line in script:\n",
    "    mtch = rg.match(line)\n",
    "    if mtch:\n",
    "        speakers.append(mtch.group())\n",
    "#Remove all line returns\n",
    "for l in range(len(speakers)):\n",
    "    speakers[l] = speakers[l].replace('\\n', '') \n",
    "    \n",
    "print(speakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11/17 START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On Frank Sinatra (1915–1998)\\n', 'Murray Kempton and Whitney Balliett JUNE 25, 1998 ISSUE\\n', '1.\\n', '\\n', 'Murray Kempton\\n', '\\n', 'The following was written in December 1996, when Frank Sinatra’s retirement was announced.\\n', '\\n', 'Frank Sinatra ever did the fullest duty to his art, and now he is leaving us with the duty to sum him up. My betters have already done that. One day I was dealing with Ella Fitzgerald, and the subject of Sinatra came up and her intruder-mistrusting voice suddenly softened and she said, “Frank. Just this little guy telling this story. That’s all you have to be.”\\n', '\\n', 'In 1956, Nelson Riddle thought to employ the Hollywood String Quartet as backup for Sinatra’s “Close to You” album. The HSQ found Sinatra as demanding as Schoenberg had been six years before, when it recorded “Verklärte Nacht” and so gratified its composer that he felt himself fully defined and registered his satisfaction by writing the liner notes. Sinatra asked not a whit less than Schoenberg, and Eleanor Aller, the HSQ’s cellist, has remembered the delight of the challenge and fulfillment as “the sort of thing in which you just enjoy every minute, because the man is so musical.”\\n', '\\n', 'ADVERTISING\\n', 'And so those made newly aware of the summit of America-bred chamber music that was the Hollywood String Quartet would do well to catch those four on “The Lady Is a Tramp” and understand that the HSQ’s glorious decade belongs not only to Schoenberg and Schubert but to Frank Sinatra, too.\\n', '\\n', 'For Frank Sinatra knew, as every artist must, that there is no such thing as trash that cannot be transcended. He also knew the second great lesson, which is that everything is there to be stolen if you have the taste to confine your larcenies to the worth-taking. We cannot appreciate the work if we overlook its elements of creative plagiarism.\\n', '\\n', 'The porkpie hat and the walk into the shadows of loneliness with the light at his back are all taken intact from the “One for My Baby” that Fred Astaire consummated in the unjustly forgotten movie The Sky’s the Limit. I remember when Sinatra was trying to put together a sextet for the Kennedy Inaugural and finally had to tell Milton Berle, “Look, Milton, the only way you’ll ever learn to sing is to listen to Billie Holiday and find out how to play out a note.” The Lord had given him his voice; mother wit and a magpie’s cunning account for the enduring distinction of the rest.\\n', '\\n', 'Our relations were always cordial, however fleeting, but it didn’t take too long to recognize the puritanical little boy beneath the skin. Once we passed a few minutes after a Kennedy rally in Los Angeles. Sinatra was in the full fit of enchainment in the great clan’s thrall, and said how great they all were: “Jack, Jackie, the ambassador, Bobby, and every one of them.”\\n', '\\n', 'He had to confess, all the same, that he was coming to doubt that Peter Lawford, the brother-in-law, had the moral fiber befitting his grand connections.\\n', '\\n', '“Do you mean to say,” I, puzzled, asked, “that, when you and Sammy Davis throw a wild night, Peter Lawford comes along?”\\n', '\\n', '“Yes,” Sinatra replied. “That’s just what I mean.”\\n', '\\n', 'And he did, because at bottom he believed—and could rise every morning and believe again—that love is eternal and fidelity is a sacred trust. There and only there was his secret. What would Don Giovanni be except merely coarse bouffe if the Don could not so unvaryingly persuade himself that each fresh object of trifling fancy is his lifetime’s love?\\n', '\\n', 'No, if you want to know why Frank Sinatra will intrude himself into the bloodstream of our memories if we survive to a hundred and five, forget the grown man he never became and look for the little boy he could not quite stop being, because that is the little boy who goes about searching for and singing about the love that will always last.\\n', '\\n', '2.\\n', '\\n', 'Whitney Balliett\\n', '\\n', 'Frank Sinatra’s singing was the only one of his careers—as a natural actor, a tough guy, a mafia friend, a womanizer, a nouveau riche, a political chameleon—that really mattered. (But he had one other largely secret career as a samaritan who was always at the ready when one of his friends—say the singer Sylvia Syms, or the composer Alec Wilder—fell ill or into trouble.) He had a microphone voice, a light baritone of barely two octaves with a slightly quavery Twenties vibrato that he learned to control almost immediately. His voice had a kind of open density, a Hoboken bel canto quality; despite his host of imitators, you invariably knew who it was one measure in. His sometimes baggy emotional range, his easy swing, shapely dynamics, and extraordinary enunciation (try to find a garbled word in a Sinatra lyric) came directly from Bing Crosby and, later in his life, Billie Holiday. But Sinatra also learned from two other disparate masters—his old boss, the trombonist and bandleader Tommy Dorsey, whose long notes hung in the air like gliders; and from the great cabaret singer Mabel Mercer, who had a three-dimensional, mother-hen way of phrasing. The writer and lyricist Gene Lees has called Sinatra our “poet laureate.” Good, but Sinatra didn’t write songs; he got inside them, found their poetry, and gave it to us.\\n', '\\n', 'Sinatra “retired” in 1971, but was back on the road a year later. In 1974, he gave a wonderful concert in the huge Nassau Coliseum on Long Island. I went to the concert, and the end of my review read: “Although Sinatra worked in the round, he remained effortless, turning in a slow-as-Earth motion that allowed every one of us to feel, no matter how briefly, his radiance.” Three weeks later—so much for his celebrated hatred of the press—a letter arrived from him. It began: “I did not plan ahead for my unretirement period, so Imust apologize for the delay in expressing my deep appreciation for your kind words.” He went on in the second and last paragraph, his own way of speaking suddenly ringing out: “It felt good—hell, it was great—to be back on stage again.”\\n', '\\n', 'Sinatra’s best singing came in two indivisible parts: the cocky, supreme voice, and his passion for the songs of Kern and Gershwin and Arlen and Berlin and Rodgers and Porter. He helped turn them into our American lieder.\\n', '\\n', 'Copyright © 1996 Newsday, Inc.\\n', '\\n', '© 1963-2018 NYREV, Inc. All rights reserved.\\n']\n"
     ]
    }
   ],
   "source": [
    "#Omit speakers Section Numbers and line numbers from the list of lines\n",
    "s = re.compile(\"(\\b[A-Z{3}\\.]+\\b)|([0-9]*\\.+\\\\*n+\\\\n)\")\n",
    "spoken = list(filter(lambda i: not s.search(i), script))\n",
    "\n",
    "    ##\\\\n\n",
    "    ##.*(\\'.*\\')\n",
    "    ##[0-9]*\\.+\\\\*n\n",
    "print(spoken)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDENTIFYING SENTENCES AND PHRASES\n",
    "\n",
    "During this step, we concatenate lines in batches to allow the identification of sentences with regular expressions. Then we identify phrases with stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate lines into list entries for future sentence splitting\n",
    "\n",
    "newLines = []\n",
    "singleLine = ''\n",
    "singleLines = []\n",
    "\n",
    "#Remove all line returns\n",
    "for j in range(0, len(spoken)):\n",
    "    spoken[j] = spoken[j].replace('\\n', '')\n",
    "    spoken[j] = spoken[j].replace('1.', '')\n",
    "    \n",
    "#Split 5 lines at a time into new list\n",
    "for k in range( 0, len(spoken), 3):\n",
    "    newLines = []\n",
    "    for line in range( 0, 3 ):\n",
    "        try:\n",
    "            newLines.append(' '+spoken[line+k])\n",
    "        except:\n",
    "            #print(\"Index Error at\", k, line)\n",
    "            break\n",
    "    #Join 5-line groups into one line and append to a list\n",
    "    singleLine = ''.join(newLines)\n",
    "    singleLines.append(singleLine)\n",
    "\n",
    "#print(singleLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A LIST OF SENTENCES FROM A TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create list of sentences\n",
    "sentences = []\n",
    "for m in range(0, len(singleLines)):\n",
    "    mtch = re.findall(\"\\A[A-Z]*[^\\.!?]*[\\.!?]\", singleLines[m], re.M|re.I)\n",
    "    if mtch:\n",
    "        sentences.append(mtch)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A LIST OF PHRASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Clean the stopword list\n",
    "stoplist = []\n",
    "clean_line = []\n",
    "data_folder = Path(\"data/\")\n",
    "file_to_open = data_folder / \"snowball_stop.txt\"\n",
    "f = open(file_to_open, 'r')\n",
    "full_stop = list(f)\n",
    "\n",
    "for n in range( 0, len(full_stop), 1 ):\n",
    "    clean_line = full_stop[n].split('|')\n",
    "    stoplist.append(clean_line[0])\n",
    "\n",
    "for p in range(len(stoplist)):\n",
    "    stoplist[p] = stoplist[p].replace('\\n', '')\n",
    "    \n",
    "#print(stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create list of phrases using stopwords\n",
    "phrases = []\n",
    "candidate_phrases = []\n",
    "\n",
    "for q in range(len(sentences)):\n",
    "    for r in sentences[q]:\n",
    "        words = re.split(\"\\\\s+\", r)\n",
    "        previous_stop = False\n",
    " \n",
    "        # Examine each word to determine if it is a phrase boundary marker or part of a phrase or alone\n",
    "        for w in words:\n",
    " \n",
    "            if w in stoplist and not previous_stop:\n",
    "                # phrase boundary encountered, so put a hard indicator\n",
    "                candidate_phrases.append(\";\")\n",
    "                previous_stop = True\n",
    "            elif w not in stoplist and len(w) > 3:\n",
    "                # keep adding words to list until a phrase boundary is detected\n",
    "                candidate_phrases.append(w.strip())\n",
    "                previous_stop = False\n",
    " \n",
    "    # Create a list of candidate phrases without boundary demarcation\n",
    "    phrases = re.split(\";+\", ' '.join(candidate_phrases))\n",
    "\n",
    "# Clean up phrases    \n",
    "re2 = re.compile('[^\\.!?,\"(){}\\*:]*[\\.!?,\"(){}\\*:]')\n",
    "for s in range(len(phrases)):\n",
    "    phrases[s] = re.sub(re2, '', phrases[s])\n",
    "    phrases[s] = phrases[s].strip(' ')\n",
    "    phrases[s] = phrases[s].replace(' ', '_')\n",
    "    phrases[s] = phrases[s].replace('__', '_')\n",
    "    phrases[s] = phrases[s].strip('_')\n",
    "\n",
    "for s in range(len(phrases)):\n",
    "    try:\n",
    "        phrases.remove('')\n",
    "        phrases.remove(' ')\n",
    "        phrases.remove('/n')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for t in range(50):\n",
    "    print(phrases[t])\n",
    "\n",
    "#print(phrases)\n",
    "# Probably we'll want to remove stop words from the list of phrases at the end of this."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERM FREQUENCY–INVERSE DOCUMENT FREQUENCY (TF-IDF)\n",
    "![image.png](attachment:image.png)\n",
    "The quintessential early Natural Language Processing tool, the TF-IDF analysis for context and sentiment evaluation is useful only over a large corpus. It must be understood that the corpus is not just a sample to be evaluated, but instead is the entire population that sets a 'benchmark' for evaluation, if you will. \n",
    "\n",
    "Here we establish a Term Frequency (TF) count of word frequencies, just as we showed a phrase frequency count in the last step in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING A WORD LIST and then a DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish wordList\n",
    "wordList = []\n",
    "for u in range(len(sentences)):\n",
    "    for v in sentences[u]:\n",
    "        words = re.split(\"\\\\s+\", v)\n",
    "        wordList.extend(words)\n",
    "        \n",
    "#for w in range(50):\n",
    "    #print(wordList[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Establish wordDict\n",
    "wordDict = {}\n",
    "for w in range(len(wordList)):\n",
    "    newWord = wordList[w]\n",
    "    newWord = newWord.lower()\n",
    "    newWord = newWord.replace('.', '')\n",
    "    wordDict[w] = newWord\n",
    "#print(wordDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORM WORD COUNT ON THE DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "countDict = {}\n",
    "for x in range(len(wordDict)):\n",
    "    term = wordDict[x]\n",
    "    count = 1\n",
    "    for y in range(len(wordDict)):\n",
    "        try:\n",
    "            if wordDict[y].find(term) > 0:\n",
    "                count += 1\n",
    "        except:\n",
    "            pass\n",
    "        countDict[term] = count\n",
    "\n",
    "for k, v in countDict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Computes ratio of word's appearances to total words\n",
    "bow = wordList\n",
    "bowCount = len(bow) #BOW = Bag of Words\n",
    "tfDict = {}\n",
    "for term, count in countDict.items():\n",
    "    tfDict[term] = count/float(bowCount)\n",
    "\n",
    "num = dict(sorted(tfDict.items(), key=lambda x: x[1], reverse = True))\n",
    "#for (k, v in num.items()):\n",
    "    #print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORM A PHRASE FREQUENCY COUNT\n",
    "\n",
    "Now we can identify common phrases by performing a frequency count on each phrase.  Moreover, if the corpus is large enough, commonly used phrases will be evident with higher counts across many texts.  For this reason the phrase list along with counts, will be stored in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase frequency count\n",
    "from operator import itemgetter\n",
    "wordfreq = []\n",
    "for u in range(len(phrases)):\n",
    "    utterance = phrases[u]\n",
    "    uttcnt = 0\n",
    "    uttcnt = phrases.count(utterance)\n",
    "    if uttcnt > 1:\n",
    "        wordfreq.append(uttcnt)\n",
    "    \n",
    "zipped = list(zip(phrases, wordfreq))\n",
    "sortzip = sorted(zipped, key=itemgetter(1), reverse=True)\n",
    "phraseFreqDict = dict(sortzip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If a sentence is usually 'subject' then 'verb' and then 'noun', we could assume that a good sentence or \n",
    "phrase (which could or also could not be a sentence), would have that structure. Simply, the common research topics \n",
    "of 'attention' 'sentiment' and 'semantics' match these structural terms in functionality, at least somewhat. \n",
    "We use a trifecta of unit vectors initially therefore to represent each of these terms, and apply them in a relatively \n",
    "intuitive fashion. Semantics are often considered ordinal and therefore a vector in 3-space could be (1,1,0). Taking a \n",
    "cartesian approach to the 3-space, Sentiment could be described as (1,0,1) when flat and Attention has been described \n",
    "as nearly orthogonal to sentiment and could therefore be (0,1,1).\n",
    "\"\"\"\n",
    "\n",
    "import xxhash, numpy\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "phraseDictH = {}\n",
    "# Estimate the size of the vocabulary\n",
    "vocabSize = len(phrases)\n",
    "docSize = len(spoken)\n",
    "phraseFreqs = phraseFreqDict.values()\n",
    "phraseVocab = phraseFreqDict.keys() \n",
    "\n",
    "# Return a dictionary whose keys are phrase tokens, the values are the indices       \n",
    "#START HERE\n",
    "for key,val in phraseFreqDict.items():\n",
    "    newPhrase = phraseFreqDict[val]\n",
    "    print(newPhrase)\n",
    "    # First Hash the Phrases\n",
    "    hashedPhrase = xxhash.xxh32(newPhrase, seed=60155748).hexdigest()\n",
    "    # Second, Pad the Hashes if they are NOT in hex (this is worth trying since hex is not ideal for text)\n",
    "    # Third, Vectorize the Padded or hex Hashes, using an embedding vector set from attention, semantics, \n",
    "    # or sentiment\n",
    "    phraseDictH[z] = hashedPhrase\n",
    "    \n",
    "# Last, One-Hot the Vectors\n",
    "hashSize = len(hashedPhrase)\n",
    "\n",
    "#print(sortzip[ v(0) ])\n",
    "\n",
    "array = numpy.zeros( (docSize, hashSize, vocabSize), dtype=float, order='F')\n",
    "for i in range(26): #This is the Z-axis    \n",
    "    for j in enumerate(phraseDictH):\n",
    "        array[i, j, phraseDictH[phraseDictH[z]]] = 1 #Here, the third param must be the IDF?\n",
    "\n",
    "result = encoder_input_data\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BYTE-LEVEL ENCODING OF THE DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = sorted(countDict.items(), key=lambda x: x[1], reverse = True)\n",
    "length = len(dict)\n",
    "#print(length)\n",
    "# Use a 0-255 integer byte level encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the encoding\n",
    "The TF-IDF and One-Hot encoder, should produce nearly the same output.  One-Hot produces a Gaussian Intensity for a phrase amongst the list of phrases, while the Inverse Document Frequency should do similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO BE CONTINUED...\n",
    "At this point, this notebook is finished. If you are really truly interested in using this code, please refer to Part 2 for a continuation of this process, wherein the code in this notebook is converted into objects and a multiple document corpus is compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
